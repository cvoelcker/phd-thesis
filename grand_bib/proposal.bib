@InProceedings{pmlr-v137-lovatto20a,
  title = 	 {Decision-Aware Model Learning for Actor-Critic Methods: When Theory Does Not Meet Practice},
  author =       {Lovatto, \^{A}ngelo G. and Bueno, Thiago P. and Mau\'{a}, Denis D. and de Barros, Leliane N.},
  booktitle = 	 {"I Can't Believe It's Not Better!" at NeurIPS Workshops},
  year = 	 {2020},
}

@inproceedings{itervaml,
 author = {Farahmand, Amir-massoud},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Iterative Value-Aware Model Learning},
 year = {2018}
}


@InProceedings{vaml,
  title = 	 {{Value-Aware Loss Function for Model-based Reinforcement Learning}},
  author = 	 {Farahmand, Amir-massoud and Barreto, Andre and Nikovski, Daniel},
  booktitle = 	 {International Conference on Artificial Intelligence and Statistics},
  year = 	 {2017},
}

@article{qlearning,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  year={1992},
  publisher={Springer}
}

@incollection{dqn,
  title = {Playing Atari With Deep Reinforcement Learning},
  author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
  booktitle = {NeurIPS Deep Learning Workshop},
  year = {2013}
}

@inproceedings{ddqn,
  title={Dueling network architectures for deep reinforcement learning},
  author={Wang, Ziyu and Schaul, Tom and Hessel, Matteo and Hasselt, Hado and Lanctot, Marc and Freitas, Nando},
  booktitle={International Conference on Machine Learning},
  year={2016},
}

@inproceedings{sac,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  year={2018},
}

@book{szepesvari_book,
author = {Szepesvári, Csaba},
year = {2010},
title = {Algorithms for Reinforcement Learning},
publisher={Morgan Claypool Publishers},
}

@incollection{dyna,
  title={Integrated architectures for learning, planning, and reacting based on approximating dynamic programming},
  author={Sutton, Richard S},
  booktitle={Machine learning Proceedings},
  year={1990},
}

@inproceedings{pets,
 author = {Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models},
 year = {2018}
}


@article{nikishin2021control,
  title={Control-Oriented Model-Based Reinforcement Learning with Implicit Differentiation},
  author={Nikishin, Evgenii and Abachi, Romina and Agarwal, Rishabh and Bacon, Pierre-Luc},
  volume = {arxiv:2106.03273},
  year={2021}
}

@inproceedings{
zhang2021learning,
title={Learning Invariant Representations for Reinforcement Learning without Reconstruction},
author={Amy Zhang and Rowan Thomas McAllister and Roberto Calandra and Yarin Gal and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2021},
}

@article{ferns2011bisimulation,
  title={Bisimulation metrics for continuous {M}arkov decision processes},
  author={Ferns, Norm and Panangaden, Prakash and Precup, Doina},
  journal={SIAM Journal on Computing},
  volume={40},
  number={6},
  year={2011},
}

@inproceedings{ferns2004metrics,
  title={Metrics for finite {M}arkov Decision Processes},
  author={Ferns, Norm and Panangaden, Prakash and Precup, Doina},
  booktitle={Uncertainty in AI},
  year={2004}
}
@inproceedings{NIPS2017_ffbd6cbb,
 author = {Oh, Junhyuk and Singh, Satinder and Lee, Honglak},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Value Prediction Network},
 year = {2017}
}

@inproceedings{10.5555/2969442.2969546,
author = {Watter, Manuel and Springenberg, Jost Tobias and Boedecker, Joschka and Riedmiller, Martin},
title = {Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images},
year = {2015},
booktitle = {Advances in Neural Information Processing Systems},
}

@inproceedings{
Levine2020Prediction,
title={Prediction, Consistency, Curvature: Representation Learning for Locally-Linear Control},
author={Nir Levine and Yinlam Chow and Rui Shu and Ang Li and Mohammad Ghavamzadeh and Hung Bui},
booktitle={International Conference on Learning Representations},
year={2020},
}

@inproceedings{
cui2021controlaware,
title={Control-Aware Representations for Model-based Reinforcement Learning},
author={Brandon Cui and Yinlam Chow and Mohammad Ghavamzadeh},
booktitle={International Conference on Learning Representations},
year={2021},
}

@article{schrittwieser2020mastering,
  title={Mastering atari, go, chess and shogi by planning with a learned model},
  author={Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
  journal={Nature},
  volume={588},
  number={7839},
  year={2020},
  publisher={Nature Publishing Group}
}
@inproceedings{grimm2021proper,
      title={Proper Value Equivalence}, 
      author={Christopher Grimm and André Barreto and Gregory Farquhar and David Silver and Satinder Singh},
      year={2021},
    booktitle = {Advances in Neural Information Processing Systems},
    pubstate={forthcoming},
    intype = {to appear in},
}


@InProceedings{lambert,
  title = 	 {Objective Mismatch in Model-based Reinforcement Learning},
  author =       {Lambert, Nathan and Amos, Brandon and Yadan, Omry and Calandra, Roberto},
  booktitle = 	 {Conference on Learning for Dynamics and Control},
  year = 	 {2020},
}

@inproceedings{mbpo,
 author = {Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {When to Trust Your Model: Model-Based Policy Optimization},
 year = {2019}
}

@article{kearns2002near,
  title={Near-optimal reinforcement learning in polynomial time},
  author={Kearns, Michael and Singh, Satinder},
  journal={Machine learning},
  volume={49},
  number={2},
  year={2002},
  publisher={Springer}
}

@article{moerland,
  author    = {Thomas M. Moerland and
               Joost Broekens and
               Catholijn M. Jonker},
  title     = {Model-based Reinforcement Learning: {A} Survey},
  volume    = {arxiv:2006.16712},
  year      = {2020}
}

@InProceedings{lambert202objective,
  title = 	 {Objective Mismatch in Model-based Reinforcement Learning},
  author =       {Lambert, Nathan and Amos, Brandon and Yadan, Omry and Calandra, Roberto},
  booktitle = 	 {Conference on Learning for Dynamics and Control},
  year = 	 {2020},
  }
  
  
@InProceedings{lovatto2020decision,
  title = 	 {Decision-Aware Model Learning for Actor-Critic Methods: When Theory Does Not Meet Practice},
  author =       {Lovatto, \^{A}ngelo G. and Bueno, Thiago P. and Mau\'{a}, Denis D. and de Barros, Leliane N.},
  booktitle = 	 {"I Can't Believe It's Not Better!" at NeurIPS Workshops},
  year = 	 {2020},
}


@inproceedings{chow2021variatiional,
  title     = {Variational Model-based Policy Optimization},
  author    = {Chow, Yinlam and Cui, Brandon and Ryu, Moonkyung and Ghavamzadeh, Mohammad},
  booktitle = {Joint Conference on Artificial Intelligence},
  year      = {2021},
}

@article{abachi2020policy,
  author    = {Romina Abachi and
               Mohammad Ghavamzadeh and
               Amir{-}massoud Farahmand},
  title     = {Policy-Aware Model Learning for Policy Gradient Methods},
  journal   = {ArXiv},
  volume = {arxiv:2003.00030},
  year      = {2020},
}

@article{asadi2018equivalence,
  title={Equivalence Between Wasserstein and Value-Aware Loss for Model-based Reinforcement Learning},
  author={Asadi, Kavosh and Cater, Evan and Misra, Dipendra and Littman, Michael L},
  journal={ArXiv},
  volume = {arxiv:1806.01265},
  year={2018}
}

@inproceedings{grimm2020value,
 author = {Grimm, Christopher and Barreto, Andre and Singh, Satinder and Silver, David},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {The Value Equivalence Principle for Model-Based Reinforcement Learning},
 year = {2020}
}

@inproceedings{nair2020goal,
  title={Goal-aware prediction: Learning to model what matters},
  author={Nair, Suraj and Savarese, Silvio and Finn, Chelsea},
  booktitle={International Conference on Machine Learning},
  year={2020},
}


@inproceedings{ross2012agnostic,
  author={Stéphane Ross and Drew Bagnell},
  title={Agnostic System Identification for Model-Based Reinforcement Learning},
  year={2012},
  booktitle={International Conference on Machine Learning},
}

@inproceedings{
luo2018algorithmic,
title={Algorithmic Framework for Model-based Deep Reinforcement Learning with Theoretical Guarantees},
author={Yuping Luo and Huazhe Xu and Yuanzhi Li and Yuandong Tian and Trevor Darrell and Tengyu Ma},
booktitle={International Conference on Learning Representations},
year={2019},
}

@inproceedings{talvitie2017self,
  title={Self-correcting models for model-based reinforcement learning},
  author={Talvitie, Erin},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2017}
}


@inproceedings{Puterman1994MarkovDP,
  title={{M}arkov Decision Processes: Discrete Stochastic Dynamic Programming},
  author={Martin L. Puterman},
  booktitle={Wiley Series in Probability and Statistics},
  year={1994}
}

@inproceedings{joseph2013reinforcement,
  title={Reinforcement learning with misspecified model classes},
  author={Joseph, Joshua and Geramifard, Alborz and Roberts, John W and How, Jonathan P and Roy, Nicholas},
  booktitle={IEEE International Conference on Robotics and Automation},
  year={2013},
}

@inproceedings{levine2013guided,
  title={Guided policy search},
  author={Levine, Sergey and Koltun, Vladlen},
  booktitle={International Conference on Machine Learning},
  year={2013},
}

@inproceedings{schneider1997exploiting,
  title={Exploiting model uncertainty estimates for safe dynamic control learning},
  author={Schneider, Jeff G},
  booktitle={Advances in Neural Information Processing Systems},
  year={1997}
}
@inproceedings{deisenroth2011pilco,
  title={PILCO: A model-based and data-efficient approach to policy search},
  author={Deisenroth, Marc and Rasmussen, Carl E},
  booktitle={International Conference on Machine Learning},
  year={2011},
}
 @article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  volume = {arxiv:1606.01540},
  year={2016}
} 

@Article{Pineda2021MBRL,
  author  = {Luis Pineda and Brandon Amos and Amy Zhang and Nathan O. Lambert and Roberto Calandra},
  title   = {MBRL-Lib: A Modular Library for Model-based Reinforcement Learning},
  year    = {2021},
  volume  = {arxiv:2104.10159}
}

@article{peters2008natural,
  title={Natural actor-critic},
  author={Peters, Jan and Schaal, Stefan},
  journal={Neurocomputing},
  volume={71},
  number={7-9},
  year={2008},
  publisher={Elsevier}
}
@incollection{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
year = {2019},
}

@article{Stone2021TheDC,
  title={The Distracting Control Suite - A Challenging Benchmark for Reinforcement Learning from Pixels},
  author={Austin Stone and Oscar Ramirez and Kurt Konolige and Rico Jonschkowski},
  year={2021},
  volume={arxiv:2101.02722}
}

@article{lutter2021learning,
      title={Learning Dynamics Models for Model Predictive Agents}, 
      author={Michael Lutter and Leonard Hasenclever and Arunkumar Byravan and Gabriel Dulac-Arnold and Piotr Trochim and Nicolas Heess and Josh Merel and Yuval Tassa},
  year={2021},
  volume={arxiv:2109.14311}
}

@inproceedings{doro2020gradient,
  title={Gradient-aware model-based policy search},
  author={D'Oro, Pierluca and Metelli, Alberto Maria and Tirinzoni, Andrea and Papini, Matteo and Restelli, Marcello},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2020}
}

@inproceedings{
Hafner2020Dream,
title={Dream to Control: Learning Behaviors by Latent Imagination},
author={Danijar Hafner and Timothy Lillicrap and Jimmy Ba and Mohammad Norouzi},
booktitle={International Conference on Learning Representations},
year={2020},
}

@book{bersekas2996neuro,
title={Neuro-Dynamic Programming},
author={Dimitri P. Bertsekas and John Tsitsiklis},
year={1996},
publisher={Athena Scientific}
}

@article{watter2015embed,
  title={Embed to control: A locally linear latent dynamics model for control from raw images},
  author={Watter, Manuel and Springenberg, Jost and Boedecker, Joschka and Riedmiller, Martin},
  journal={Advances in neural information processing systems},
  year={2015}
}


@inproceedings{levine2020Prediction,
title={Prediction, Consistency, Curvature: Representation Learning for Locally-Linear Control},
author={Nir Levine and Yinlam Chow and Rui Shu and Ang Li and Mohammad Ghavamzadeh and Hung Bui},
booktitle={International Conference on Learning Representations},
year={2020},
}

@inproceedings{hafner2019learning,
  title={Learning latent dynamics for planning from pixels},
  author={Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
  booktitle={International Conference on Machine Learning},
  year={2019},
}

@article{bromley1993signature,
  title={Signature verification using a “siamese” time delay neural network},
  author={Bromley, Jane and Bentz, James W and Bottou, L{\'e}on and Guyon, Isabelle and LeCun, Yann and Moore, Cliff and S{\"a}ckinger, Eduard and Shah, Roopak},
  journal={International Journal of Pattern Recognition and Artificial Intelligence},
  year={1993},
}

@article{grill2020bootstrap,
  title={Bootstrap your own latent-a new approach to self-supervised learning},
  author={Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and others},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}

@article{oh2017value,
  title={Value prediction network},
  author={Oh, Junhyuk and Singh, Satinder and Lee, Honglak},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{vagram,
  title={Value Gradient weighted Model-Based Reinforcement Learning},
  author={Voelcker, Claas and Liao, Victor and Garg, Animesh and Amir-massoud},
  journal={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{silver2017predictron,
  title={The predictron: End-to-end learning and planning},
  author={Silver, David and Hasselt, Hado and Hessel, Matteo and Schaul, Tom and Guez, Arthur and Harley, Tim and Dulac-Arnold, Gabriel and Reichert, David and Rabinowitz, Neil and Barreto, Andre and others},
  booktitle={International Conference on Machine Learning},
  year={2017},
}

@article{tomar2021learning,
  title={Learning Representations for Pixel-based Control: What Matters and Why?},
  author={Tomar, Manan and Mishra, Utkarsh A and Zhang, Amy and Taylor, Matthew E},
  volume = {arxiv:2111.07775},
  year      = {2021},
}

@article{brunton2021modern,
  title={Modern Koopman theory for dynamical systems},
  author={Brunton, Steven L and Budi{\v{s}}i{\'c}, Marko and Kaiser, Eurika and Kutz, J Nathan},
  volume = {arxiv:2102.12086},
  year      = {2021},
}

@inproceedings{
chen2021decision,
title={Decision Transformer: Reinforcement Learning via Sequence Modeling},
author={Lili Chen and Kevin Lu and Aravind Rajeswaran and Kimin Lee and Aditya Grover and Michael Laskin and Pieter Abbeel and Aravind Srinivas and Igor Mordatch},
booktitle={Advances in Neural Information Processing Systems},
year={2021},
}

@inproceedings{rajeswaran2020game,
  title={A game theoretic framework for model based reinforcement learning},
  author={Rajeswaran, Aravind and Mordatch, Igor and Kumar, Vikash},
  booktitle={International Conference on Machine Learning},
  year={2020},
}

@article{osband2016deep,
  title={Deep exploration via bootstrapped DQN},
  author={Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
  journal={Advances in neural information processing systems},
  year={2016}
}

@inproceedings{
chen2021an,
title={An Empirical Investigation of Representation Learning for Imitation},
author={Xin Chen and Sam Toyer and Cody Wild and Scott Emmons and Ian Fischer and Kuang-Huei Lee and Neel Alex and Steven H Wang and Ping Luo and Stuart Russell and Pieter Abbeel and Rohin Shah},
booktitle={Neural Information Processing Systems Datasets and Benchmarks Track},
year={2021},
}

 @book{suttonbarto,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@inproceedings{riedmiller2005neural,
  title={Neural fitted Q iteration--first experiences with a data efficient neural reinforcement learning method},
  author={Riedmiller, Martin},
  booktitle={European Conference on Machine Learning},
  year={2005}
  }

@inproceedings{pan2019hill,
  title={Hill climbing on value estimates for search-control in Dyna},
  author={Pan, Yangchen and Yao, Hengshuai and Farahmand, Amir-Massoud and White, Martha},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={2019}
}

@inproceedings{metaworld,
  title={Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning},
  author={Yu, Tianhe and Quillen, Deirdre and He, Zhanpeng and Julian, Ryan and Hausman, Karol and Finn, Chelsea and Levine, Sergey},
  booktitle={Conference on Robot Learning},
  year={2020},
}

@inproceedings{wilson2007multi,
  title={Multi-task reinforcement learning: a hierarchical bayesian approach},
  author={Wilson, Aaron and Fern, Alan and Ray, Soumya and Tadepalli, Prasad},
  booktitle={International Conference on Machine Learning},
  year={2007}
}

@article{ernst2005tree,
  title={Tree-based batch mode reinforcement learning},
  author={Ernst, Damien and Geurts, Pierre and Wehenkel, Louis},
  journal={Journal of Machine Learning Research},
  volume={6},
  pages={503--556},
  year={2005},
  publisher={Microtome Publishing}
}

@article{farahmand2010error,
  title={Error propagation for approximate policy and value iteration},
  author={Farahmand, Amir-massoud and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
  journal={Advances in Neural Information Processing Systems},
  year={2010}
}

@inproceedings{gordon1995stable,
  title={Stable Function Approximation in Dynamic Programming},
  author={Gordon, Geoffrey J},
  booktitle={International Conference on Machine Learning},
  year={1995}
}
@phdthesis{abel2020thesis,
title={A Theory of Abstraction in Reinforcement Learning},
author={David Abel},
school={Brown University},
year={2020}
}

@inproceedings{le2022generalization,
  title={On the Generalization of Representations in Reinforcement Learning},
  author={Le Lan, Charline and Tu, Stephen and Oberman, Adam and Agarwal, Rishabh and Bellemare, Marc G},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2022},
}

@inproceedings{le2021metrics,
  title={Metrics and continuity in reinforcement learning},
  author={Le Lan, Charline and Bellemare, Marc G and Castro, Pablo Samuel},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2021}
}

@inproceedings{ghosh2020representations,
  title={Representations for stable off-policy reinforcement learning},
  author={Ghosh, Dibya and Bellemare, Marc G},
  booktitle={International Conference on Machine Learning},
  year={2020},
}

@inproceedings{voelcker2023lambda,
      title={$\lambda$-AC: Learning latent decision-aware models for reinforcement learning in continuous state-spaces}, 
      author={Claas A Voelcker and Arash Ahmadian and Romina Abachi and Igor Gilitschenski and Amir-massoud Farahmand},
      year={2023},
        booktitle={under review},
}

@ARTICLE {bengio2012representation,
author = {Y. Bengio and A. Courville and P. Vincent},
journal = {IEEE Transactions on Pattern Analysis & Machine Intelligence},
title = {Representation Learning: A Review and New Perspectives},
year = {2013},
publisher = {IEEE Computer Society},
}

 @ARTICLE{hasselt2018reinforcement,
   title         = "Deep Reinforcement Learning and the Deadly Triad",
   author        = "van Hasselt, Hado and Doron, Yotam and Strub, Florian and
                    Hessel, Matteo and Sonnerat, Nicolas and Modayil, Joseph",
   year          =  2018,
   archivePrefix = "arXiv",
   primaryClass  = "cs.AI",
   eprint        = "1812.02648"
 }
 

 @INPROCEEDINGS{mikolov2013distirbuted,
   title     = "Distributed Representations of Words and Phrases and their
                Compositionality",
   booktitle = "Advances in Neural Information Processing Systems",
   author    = "Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado,
                Greg S and Dean, Jeff",
   year      =  2013
 }


@ARTICLE{goldberg2014word2vec,
  title         = "word2vec Explained: deriving Mikolov et al.'s
                   negative-sampling word-embedding method",
  author        = "Goldberg, Yoav and Levy, Omer",
  abstract      = "The word2vec software of Tomas Mikolov and colleagues
                   (https://code.google.com/p/word2vec/ ) has gained a lot of
                   traction lately, and provides state-of-the-art word
                   embeddings. The learning models behind the software are
                   described in two research papers. We found the description
                   of the models in these papers to be somewhat cryptic and
                   hard to follow. While the motivations and presentation may
                   be obvious to the neural-networks language-modeling crowd,
                   we had to struggle quite a bit to figure out the rationale
                   behind the equations. This note is an attempt to explain
                   equation (4) (negative sampling) in ``Distributed
                   Representations of Words and Phrases and their
                   Compositionality'' by Tomas Mikolov, Ilya Sutskever, Kai
                   Chen, Greg Corrado and Jeffrey Dean.",
  month         =  feb,
  year          =  2014,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1402.3722"
}


@INPROCEEDINGS{nikishin2022primacy,
  title     = "The Primacy Bias in Deep Reinforcement Learning",
  booktitle = "International Conference on Machine Learning",
  author    = "Nikishin, Evgenii and Schwarzer, Max and D'Oro, Pierluca and
               Bacon, Pierre-Luc and Courville, Aaron",
  pages     = "16828--16847",
  year      =  2022
}
@inproceedings{
kumar2022dr,
title={{DR}3: Value-Based Deep Reinforcement Learning Requires Explicit Regularization},
author={Aviral Kumar and Rishabh Agarwal and Tengyu Ma and Aaron Courville and George Tucker and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2022},
}
@inproceedings{lyle2021effect,
  title={On the effect of auxiliary tasks on representation dynamics},
  author={Lyle, Clare and Rowland, Mark and Ostrovski, Georg and Dabney, Will},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2021},
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Lyle2022-st,
  title     = "Understanding and preventing capacity loss in reinforcement
               learning",
  author    = "Lyle, Clare and Rowland, Mark and Dabney, Will",
  journal   = "arXiv:2204.09560",
  year      =  2022,
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@inproceedings{lyle2021earning,
  title={Learning dynamics and generalization in deep reinforcement learning},
  author={Lyle, Clare and Rowland, Mark and Dabney, Will and Kwiatkowska, Marta and Gal, Yarin},
  booktitle={International Conference on Machine Learning},
  year={2022},
}


@ARTICLE{Bellemare2019-eo,
  title={A geometric perspective on optimal representations for reinforcement learning},
  author={Bellemare, Marc and Dabney, Will and Dadashi, Robert and Ali Taiga, Adrien and Castro, Pablo Samuel and Le Roux, Nicolas and Schuurmans, Dale and Lattimore, Tor and Lyle, Clare},
  journal={Advances in neural information processing systems},
  year={2019}
}



@article{barreto2017successor,
  title={Successor features for transfer in reinforcement learning},
  author={Barreto, Andr{\'e} and Dabney, Will and Munos, R{\'e}mi and Hunt, Jonathan J and Schaul, Tom and van Hasselt, Hado P and Silver, David},
  journal={Advances in neural information processing systems},
  year={2017}
}

@inproceedings{
borsa2018universal,
title={Universal Successor Features Approximators},
author={Diana Borsa and Andre Barreto and John Quan and Daniel J. Mankowitz and Hado van Hasselt and Remi Munos and David Silver and Tom Schaul},
booktitle={International Conference on Learning Representations},
year={2019},
}

@inproceedings{
jaderberg2017reinforcement,
title={Reinforcement Learning with Unsupervised Auxiliary Tasks},
author={Max Jaderberg and Volodymyr Mnih and Wojciech Marian Czarnecki and Tom Schaul and Joel Z Leibo and David Silver and Koray Kavukcuoglu},
booktitle={International Conference on Learning Representations},
year={2017},
}

@inproceedings{
farebrother2023protovalue,
title={Proto-Value Networks: Scaling Representation Learning with Auxiliary Tasks},
author={Jesse Farebrother and Joshua Greaves and Rishabh Agarwal and Charline Le Lan and Ross Goroshin and Pablo Samuel Castro and Marc G Bellemare},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
}


@INPROCEEDINGS{Laskin2020-af,
  title     = "{{CURL}}: Contrastive Unsupervised Representations for
               Reinforcement Learning",
  booktitle = "International Conference on Machine Learning",
  author    = "Laskin, Michael and Srinivas, Aravind and Abbeel, Pieter",
  year      =  2020
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Gelada2019-ti,
  title    = "{DeepMDP}: Learning Continuous Latent Space Models for
              Representation Learning",
  author   = "Gelada, Carles and Kumar, Saurabh and Buckman, Jacob and Nachum,
              Ofir and Bellemare, Marc G",
  abstract = "Many reinforcement learning (RL) tasks provide the agent with
              high-dimensional observations that can be simpliﬁed into
              low-dimensional continuous states. To formalize this process, we
              introduce the concept of a DeepMDP, a parameterized latent space
              model that is trained via the minimization of two tractable
              losses: prediction of rewards and prediction of the distribution
              over next latent states. We show that the optimization of these
              objectives guarantees (1) the quality of the latent space as a
              representation of the state space and (2) the quality of the
              DeepMDP as a model of the environment. We connect these results
              to prior work in the bisimulation literature, and explore the use
              of a variety of metrics. Our theoretical ﬁndings are
              substantiated by the experimental result that a trained DeepMDP
              recovers the latent structure underlying high-dimensional
              observations on a synthetic environment. Finally, we show that
              learning a DeepMDP as an auxiliary task in the Atari 2600 domain
              leads to large performance improvements over model-free RL.",
  journal  = "arXiv:1906.02736",
  year     =  2019,
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Schwarzer2021-ew,
  title    = "Data-efficient Reinforcement Learning with Self-predictive Representations",
  author   = "Schwarzer, Max and Anand, Ankesh and Goel, Rishab and Hjelm, R
              Devon and Courville, Aaron and Bachman, Philip",
  booktitle={International Conference on Learning Representations},
  year     =  2021
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@inproceedings{Schwarzer2021-if,
title={Pretraining Representations for Data-Efficient Reinforcement Learning},
author={Max Schwarzer and Nitarshan Rajkumar and Michael Noukhovitch and Ankesh Anand and Laurent Charlin and R Devon Hjelm and Philip Bachman and Aaron Courville},
booktitle={Advances in Neural Information Processing Systems},
year={2021},
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Tang2023-ah,
  title     = "Understanding self-predictive learning for reinforcement
               learning",
  author    = "Tang, Y and Guo, Z D and Richemond, P H and {others}",
  abstract  = "We study the learning dynamics of self-predictive learning for
               reinforcement learning, a family of algorithms that learn
               representations by minimizing the prediction error of their own
               …",
  journal   = "International",
  publisher = "proceedings.mlr.press",
  year      =  2023
}

@inproceedings{kemertas2021towards,
author = {Kemertas, Mete and Aumentado-Armstrong, Tristan},
booktitle = {Advances in Neural Information Processing Systems},
title = {Towards Robust Bisimulation Metric Learning},
year = {2021}
}

@inproceedings{kastner2021mico,
author = {Castro, Pablo Samuel and Kastner, Tyler and Panangaden, Prakash and Rowland,
Mark},
booktitle = {Advances in Neural Information Processing Systems},
title = {MICo: Improved representations via sampling-based state similarity for Markov
decision processes},
year = {2021}
}


@InProceedings{amos2021modelbased,
  title = 	 {On the Model-Based Stochastic Value Gradient for Continuous Reinforcement Learning learning},
  author =       {Amos, Brandon and Stanton, Samuel and Yarats, Denis and Wilson, Andrew Gordon},
  booktitle = 	 {Conference on Learning for Dynamics and Control},
  year = 	 {2021},
}

@inproceedings{
viper,
title={{VIP}er: Iterative Value-Aware Model Learning on the Value Improvement Path},
author={Romina Abachi and Claas A Voelcker and Animesh Garg and Amir-massoud Farahmand},
booktitle={Decision Awareness in Reinforcement Learning Workshop at ICML 2022},
year={2022},
}

@article{ye2021mastering,
  title={Mastering atari games with limited data},
  author={Ye, Weirui and Liu, Shaohuai and Kurutach, Thanard and Abbeel, Pieter and Gao, Yang},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}

@inproceedings{tdmpc,
	title={Temporal Difference Learning for Model Predictive Control},
	author={Nicklas Hansen and Xiaolong Wang and Hao Su},
	booktitle={International Conference on Machine Learning},
	year={2022}
}

@article{tunyasuvunakool2020,
         title = {dm\_control: Software and tasks for continuous control},
         journal = {Software Impacts},
         volume = {6},
         year = {2020},
         author = {Saran Tunyasuvunakool and Alistair Muldal and Yotam Doron and
                   Siqi Liu and Steven Bohez and Josh Merel and Tom Erez and
                   Timothy Lillicrap and Nicolas Heess and Yuval Tassa},
}

