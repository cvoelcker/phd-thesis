\chapter{Formal proofs and results}

\todo[inline]{Move to main body where appropriate}

\section{Bound between Value-Aware Model Learning and VaGraM}
\label{app:taylor_bound}

The error in Taylor approximation $\mathcal{R}(V, s', f_\theta(s,a))$ is bounded by $\frac{M}{2}||s' - f_\theta(s,a)||^2$ with M dependent on the Hessian of the value function. Plugging this into the VAML loss and assuming worst case approximation errors, we obtain an upper bound on the VAML error:

\begin{align*}
    &\E_{s',s,a \sim D}\left[\left(V\left(f_\theta(s,a) \right) - V(s_0')\right)^2\right] \\
    = &\E_{s',s,a \sim D}\left[\left((\nabla_s V(s)|_{s'})^\intercal (f_\theta(s,a) - s') + \mathcal{R}(V, s',f_\theta(s,a))\right)^2\right]\\
    \leq &\E_{s',s,a \sim D}\left[\left(\left|(\nabla_s V(s)|_{s'})^\intercal (f_\theta(s,a) - s')\right| + \left|\mathcal{R}(V, s_0', f_\theta(s,a))\right|\right)^2\right]\\
    \leq &\E_{s',s,a \sim D}\left[{\left(\left|(\nabla_s V(s)|_{s'})^\intercal (f_\theta(s,a) - s')\right| + \frac{M}{2}||s' - f_\theta(s,a)||^2\right)^2}\right]\\
    \leq &2\cdot\E_{s',s,a \sim D}\left[{\left((\nabla_s V(s)|_{s'})^\intercal (f_\theta(s,a) - s')\right)^2}\right] + 2\cdot\E_{s',s,a \sim D}\left[\frac{M^2}{4}||s' - f_\theta(s,a)||^4\right]
\end{align*}

Our experiments show that if we treat $M$ like a tuneable hyperparameter, we obtain worse performance when optimizing this upper bound compared to  VaGraM.
The Hessian parameter is difficult to compute or estimate in practice and we find that most often, either the first or the second loss component will dominate when choosing heuristic values.

\section{Analyzing the additional local minima of the Taylor approximation loss}

We noted in the main paper that the direct Taylor approximation of the value function leads to a spurious local minimum. This is clear when looking at the loss for a single datapoint:
\begin{align*}
    &\min_\theta \left((\nabla_s V(s)|_{s'})^\intercal f_\theta(s,a) \right) ^2\\
    =&\min_\theta \left(\sum_{n=0}^{\text{dim}(\mathcal{S})} (\nabla_s V(s)|_{s'})_n \cdot f_\theta(s,a)_n\right)^2
\end{align*}

Assuming that $f$ is flexible and can predict any next state $s'$ (i.e. by choosing $f=\theta$), the optimal solution is obtained from an undetermined linear system of equations.
This system admits far more solutions than either the corresponding IterVAML loss or a mean squared error, and many of them will achieve arbitrary large value prediction errors. In fact, the equation describes a hyperplane of minimal solutions consisting of every weight vector that is orthogonal to the gradient of the value function at the reference sample, with $\text{dim}(\mathcal{S}) - 1$ free variables. Therefore we need to enforce the closeness of the model prediction and the environment sample, since the Taylor approximation is only approximately valid in a close ball around the reference sample.

One way to achieve this closeness is by adding the second order Taylor term, which results in an additional MSE loss term.
As pointed out in \autoref{app:taylor_bound}, we did not achieve good performance when testing out this version, since it is difficult to compute the Hessian in higher state spaces and heuristically choosing a value as a hyperparameter proved to be difficult to tune in practice.
Therefore, we approached the solution to this problem as outlined in the paper.

\section{Proofs and mathematical clarifications}
\label{app:proofs}

We provide the proofs for \autoref{sec:theory} in this section.


\newcommand{\mx}[1]{\hat{x}^{(#1)}_i}
\newcommand{\px}[1]{x^{(#1)}_i}
\newcommand{\TrV}{\squareb{r\roundb{x^{(1)}_i} + \gamma V'\roundb{x^{(2)}_{i}} }}
\newcommand{\TrVc}{\squareb{r\roundb{x^{(1)}_i} + \gamma \hat{V}\roundb{x^{(2)}_{i}} }}
\newcommand{\TrVp}{\squareb{r\roundb{x^{(1)}_i} + \gamma V\roundb{x^{(2)}_{i}} }}
\newcommand{\Vest}{V_{\text{est}}\roundb{\px{1},\px{2}}}
\setcounter{proposition}{0}
% \newtheorem{conjecture}{Conjecture}

\label{app:conjecture}
The first proposition relies on the existence of a deterministic mapping, which we prove here as a lemma.
The second proposition requires a statement over the minimizers of an equation with a variance-like term, we prove a general result as a lemma.

The proof of the first lemma relies heavily on several propositions from~\cite{bertsekasshreve1978}, which are restated in \autoref{sec:bertsekas} for reader's convenience.
Other topological statements are standard and can be find in textbooks such as~\cite{Munkres2018}.

\begin{lemma}[Deterministic Representation Lemma]
\label{lem:deterministic_representation_lemma}
    Let $\mathcal{X}$ be a compact, connected, metrizable space. Let $p$ be a continuous kernel from $\mathcal{X}$ to probability measures over $\mathcal{X}$. Let $\mathcal{Z}$ be a metrizable space. Consider a bijective latent mapping $\phi: \mathcal{X} \rightarrow \mathcal{Z}$ and any $V: \mathcal{Z} \rightarrow \mathbb{R}$. Assume that they are both continuous. Denote $V_\mathcal{X} = V \circ \phi$.
    
    Then there exists a measurable function $f^*: \mathcal{Z} \rightarrow \mathcal{Z}$ such that we have $V(f^*(\phi(x))) = \EEX{p}{V_\mathcal{X}(x')|x}$ for all $x \in \mathcal{X}$.
\end{lemma}

\paragraph{Proof:}
Since $\phi$ is a bijective continuous function over a compact space and maps to a Hausdorff space ($\mathcal{Z}$ is metrizable, which implies Hausdorff), it is a homeomorphism.
The image of $\mathcal{X}$ under $\phi$, $\mathcal{Z}_\mathcal{X}$ is then connected and compact.
Since $\mathcal{X}$ is metrizable and compact and $\phi$ is a homeomorphism, $\mathcal{Z}_\mathcal{X}$ is metrizable and compact.
Let $\theta_{V,\mathcal{X}}(x) = \EEX{x' \sim p(\cdot|x)}{V(x')}$.
Then, $\theta_{V,\mathcal{X}}$ is continuous (Proposition \autoref{prop:730}).
Define $\theta_{V, \mathcal{X}} = \theta_{V, \mathcal{Z}} \circ \phi$.
Since $\phi$ is a homeomorphism, $\phi^{-1}$ is continuous.
The function $\theta_{V, \mathcal{Z}}$ can be represented as a composition of continuous functions $\theta_{V, \mathcal{Z}} = \theta_{V, \mathcal{X}} \circ \phi^{-1}$ and is therefore continuous.

As $\mathcal{Z}_\mathcal{X}$ is compact, the continuous function $V$ takes a maximum and minimum over the set $\mathcal{Z}_\mathcal{X}$. 
This follows from the compactness of $\mathcal{Z}_\mathcal{X}$ and the extreme value theorem. 
Furthermore $V_{\min} \leq \theta_{V,\mathcal{Z}}(z) \leq V_{\max}$ for every $z \in \mathcal{Z}_\mathcal{X}$.
By the intermediate value theorem over compact, connected spaces, and the continuity of $V$, for every value $V_{\min} \leq v \leq V_{\max}$, there exists a $z \in \mathcal{Z}_\mathcal{X}$ so that $V(z) = v$.


Let $h: \mathcal{Z}_\mathcal{X} \times \mathcal{Z}_\mathcal{X} \rightarrow \mathbb{R}$ be the function $h(z,z') = \abs{\theta_{V,\mathcal{Z}}(z) - V(z')}^2$.
As $h$ is a composition of continuous functions, it is itself continuous.
Let $h^*(z) = \min_{z' \in \mathcal{Z}_\mathcal{X}} h(z,z')$.
For any $z \in \mathcal{Z}_\mathcal{X}$, by the intermediate value argument, there exist $z'$ such that $V(z') = v$. 
Therefore $h^*(z)$ can be minimized perfectly for all $z \in \mathcal{Z}_\mathcal{X}$.

Since $\mathcal{Z}_\mathcal{X}$ is compact, $h$ is defined over a compact subset of $\mathcal{Z}$.
By Proposition \autoref{prop:733}, there exists a measurable function $f^*(z)$ so that $\min_{z'} h(z, z') = h(z, f^*(z)) = 0$.
Therefore, the function $f^*$ has the property that $V(f^*(z)) = \EEX{p}{V(z')|z}$, as this minimizes the function $h$.

Now consider any $x\in\mathcal{X}$ and its corresponding $z=\phi(x)$.
As $h(z, f^*(z))=\abs{\theta_{V,\mathcal{Z}}(z) - V\roundb{f^*(z)}}^2 = 0$ for any $z \in \mathcal{Z}_\mathcal{X}$, $V(f^*(\phi(x))) = \theta_{v,\mathcal{Z}}(z) = \EEX{p}{V_\mathcal{X}(x')|x}$ as desired.

\hfill \ensuremath{\Box{}}

The following lemma shows that a function that minimizes a quadratic and a variance term cannot be the minimum function of the quadratic. This is used to show that the minimum of the MuZero value function learning term is not the same as applying the model-based Bellman operator.

\begin{lemma}
Let $g: \mathcal{X} \rightarrow \mathbb{R}$ be a function that is not constant almost everywhere and let $\mu$ be a non-degenerate probability distribution over $\mathcal{X}$. Let $\mathcal{F}$ be an open function space with $g \in \mathcal{F}$. Let $\mathcal{L}\roundb{f} = \mathbb{E}_{x\sim\mu}\squareb{\roundb{f(x) - g(x)}^2} + \mathbb{E}_{x\sim\mu}\squareb{f(x) g(x)} - \mathbb{E}_{x\sim\mu}\squareb{f(x)}\mathbb{E}_\mu\squareb{g(x)}$.
Then $g \notin \argmin_{f \in \mathcal{F}} \mathcal{L}(f)$.
\end{lemma}

\paragraph{Proof:}
The proof follows by showing that there is a descent direction from $g$ that improves upon $\mathcal{L}$. For this, we construct the auxiliary function $\hat{g}(x) = g(x) - \epsilon g(x)$.
Substituting $\hat{g}$ into $\mathcal{L}$ yields 
\begin{align*}
     &~\epsilon^2 \mathbb{E}_\mu\squareb{g(x)^2} + \mathbb{E}_\mu\squareb{\roundb{g(x) - \epsilon g(x)} g(x)} - \mathbb{E}_\mu\squareb{\roundb{g(x) - \epsilon g(x)}}\mathbb{E}_\mu\squareb{g(x)}\\
     =&~\epsilon^2 \mathbb{E}_\mu\squareb{g(x)^2} + \roundb{1-\epsilon}\mathbb{E}_\mu\squareb{g(x)^2} - \roundb{1 - \epsilon} \mathbb{E}_\mu\squareb{g(x)}^2.
\end{align*}

Taking the derivative of this function wrt to $\epsilon$ yields
\begin{align*}
     &~\frac{\mathrm{d}}{\mathrm{d} \epsilon}\epsilon^2 \mathbb{E}_\mu\squareb{g(x)^2} + \roundb{1-\epsilon}\mathbb{E}_\mu\squareb{g(x)^2} - \roundb{1 - \epsilon} \mathbb{E}_\mu\squareb{g(x)}^2\\
     =&~2\epsilon~\mathbb{E}_\mu\squareb{g(x)^2} -\mathbb{E}_\mu\squareb{g(x)^2} + \mathbb{E}_\mu\squareb{g(x)}^2.
\end{align*}

Setting $\epsilon$ to 0, obtain
\begin{align*}
     &~\mathbb{E}_\mu\squareb{g(x)}^2 - \mathbb{E}_\mu\squareb{g(x)^2} = \text{Var}_\mu\squareb{g(x)}
\end{align*}

By the Cauchy-Schwartz inequality, the variance is only $0$ for a $g(x)$ constant almost everywhere. However, this violates the assumption.
Therefore for any $\epsilon > 0$ $\mathcal{L}\roundb{\hat{g}} \leq \mathcal{L}\roundb{g}$, due to the descent direction given by $-g(x)$.
As we assume that the function class is open, there also exists an $\epsilon > 0$ for which $g(x) - \epsilon g(x) \in \mathcal{F}$.

\hfill \ensuremath{\Box}


\subsection{Main propositions}
\label{app:main_proofs}

\begin{proposition}
    Let $M$ be an MDP with a compact, connected state space $\mathcal{X} \subseteq \mathcal{Y}$, where $\mathcal{Y}$ is a metrizable space. Let the transition kernel $p$ be continuous. Let $\mathcal{Z}$ be a metrizable space. Consider a bijective latent mapping $\phi: \mathcal{Y} \rightarrow \mathcal{Z}$ and any value function approximation $V: \mathcal{Z} \rightarrow \mathbb{R}$. Assume that they are both continuous. Denote $V_\mathcal{X} = V \circ \phi$.
    
    Then there exists a measurable function $f^*: \mathcal{Z} \rightarrow \mathcal{Z}$ such that we have $V(f^*(\phi(x))) = \EEX{p}{V_\mathcal{X}(x')|x}$ for all $x \in \mathcal{X}$.
    
    Furthermore, the same $f^*$ is a minimizer of the population IterVAML loss:
    \begin{align*}
        f^* \in \argmin_{\hat{f}} \EEX{}{\hat{\mathcal{L}}_\text{IterVAML}(\hat{f};V_\mathcal{X})}.
    \end{align*}
\end{proposition}

\paragraph{Proof:}
The existence of $f^*$ follows under the stated assumptions (compact, connected and metrizable state space, metrizable latent space, continuity of all involved functions) from Lemma \autoref{lem:deterministic_representation_lemma}.

The rest of the proof follows standard approaches in textbooks such as \cite{Gyrfi2002ADT}. 
First, expand the equation to obtain:
\begin{align*}
    \EEX{}{\hat{\mathcal{L}}^n_\text{IterVAML}(f; V_\mathcal{X}, \mathcal{D})} &= \EEX{}{\frac{1}{N}\sum_{i=1}^N\squareb{V\roundb{f\roundb{\phi(x_i)}} - V_\mathcal{X}(x'_i) }^2}\\
    &= \EEX{}{\squareb{V\roundb{f\roundb{\phi(x)}} - \EEX{}{V_\mathcal{X}(x')\middle|x} + \EEX{}{V_\mathcal{X}(x')\middle|x} - V_\mathcal{X}(x') }^2}.
\end{align*}
After expanding the square, we obtain three terms:
\begin{align*}
    &\EEX{}{\left|{V\roundb{f\roundb{\phi(x)}} - \EEX{}{V_\mathcal{X}(x')\middle|x}}\right|^2} \\
+ &2\EEX{}{\squareb{V\roundb{f\roundb{\phi(x)}} - \EEX{}{V_\mathcal{X}(x')\middle|x}}\squareb{\EEX{}{V_\mathcal{X}(x')\middle|x} - V_\mathcal{X}(x') }}\\
    + &\EEX{}{\left|{\EEX{}{V_\mathcal{X}(x')\middle|x} - V_\mathcal{X}(x') }\right|^2}\\
\end{align*}

Apply the tower property to the inner term to obtain: 
\begin{align*}
&2\EEX{}{\squareb{V\roundb{f\roundb{\phi(x)}} - \EEX{}{V_\mathcal{X}(x')\middle|x}}\squareb{\EEX{}{V_\mathcal{X}(x')\middle|x} - V_\mathcal{X}(x') }}\\
= &2\EEX{}{\squareb{V\roundb{f\roundb{\phi(x)}} - \EEX{}{V_\mathcal{X}(x')\middle|x}}\underbrace{\EEX{}{\EEX{}{V_\mathcal{X}(x')\middle|x} - V_\mathcal{X}(x') \middle|x'}}_{=0}} = 0.
\end{align*}

Since the statement we are proving only applies to the minimum of the IterVAML loss, we will work with the $\argmin$ of the loss function above.
The resulting equation contains a term dependent on $f$ and one independent of $f$:
\begin{align*}
    \argmin_f~ & \EEX{}{\left|{V\roundb{f\roundb{\phi(x)}} - \EEX{}{V_\mathcal{X}(x')\middle|x}}\right|^2} + \EEX{}{\left|{\EEX{}{V_\mathcal{X}(x')\middle|x} - V_\mathcal{X}(x') }\right|^2}\\
    =~ \argmin_f~ & \EEX{}{\left|{V\roundb{f\roundb{\phi(x)}} - \EEX{}{V_\mathcal{X}(x')\middle|x}}\right|^2}.
\end{align*}

Finally, it is easy to notice that $V\roundb{f^*\roundb{\phi(x)}} = \EEX{}{V_\mathcal{X}(x')\middle|x}$ by the definition of $f^*$.
Therefore $f^*$ minimizes the final loss term and, due to that, the IterVAML loss.

\hfill \ensuremath{\Box}


\begin{proposition}
    Assume a non-deterministic MDP with a fixed, but arbitrary policy $\pi$, and let $p$ be the transition kernel. 
    % Assume that the MDP and $\pi$ are such that the value function $V^\pi$ is not constant almost everywhere.
    % Let $\mathcal{D}$ be a dataset of tuples $\{s,a,r,s'\}$ collected from a distribution that covers the state-action space fully. 
    Let $\mathcal{V}$ be an open set of functions, and assume that it is Bellman complete: $\forall V \in \mathcal{V}: \mathcal{T}V \in \mathcal{V}$.
    
    \rev{Then for any $V' \in \mathcal{V}$ that is not a constant function, $ \mathcal{T}V' \notin \argmin_{\hat{V} \in \mathcal{V}} \mathbb{E}_{\mathcal{D}}\squareb{\hat{\mathcal{L}}_\text{MuZero}^1(p, \hat{V}; \mathcal{D}, V')}$.}
\end{proposition}

\paragraph{Notation:}

\newcommand{\target}{\roundb{\mathcal{T}V'}\roundb{\mx{1}}}

For clarity of presentation denote samples from the real environment as $x^{(n)}$ for the $n$-th sample after a starting point $x^{(0)}$. 
This means that $x^{(n+1)}$ is drawn from $p\roundb{\cdot\middle|x^{(n)}}$. Similarly, $\hat{x}^{(n)}$ is the $n$-th sample drawn from the model, with $\hat{x}^{(0)} = x^{(0)}$.
All expectations are taken over $\px{0} \sim \mu$ where $\mu$ is the data distribution, $\mx{1} \sim \hat{p}\roundb{\cdot\middle|\px{0}}$, $\px{1} \sim p\roundb{\cdot\middle|\px{0}}$, and $\px{2} \sim p\roundb{\cdot\middle|\px{1}}$.
We use the tower property several times, all expectations are conditioned on ${\px{0}}_i$.


\paragraph{Proof:}

By assumption, let $\hat{p}$ in the MuZero loss be the true transition kernel $p$. Expand the MuZero loss by $\TrV$ and take its expectation:
\begin{align}
    &\EEX{}{\hat{\mathcal{L}}^1_\text{MuZero}(\hat{p}, \hat{V}; \mathcal{D}, V')}\nonumber\\ 
  =& \EEX{}{\frac{1}{N}\sum_{i=1}^N \squareb{ \hat{V}\roundb{\mx{1}} - \TrV }^2}\nonumber\\
  = &~\EEX{}{\squareb{ \hat{V}\roundb{\mx{1}} - \target + \target - \TrV  }^2}\nonumber\\
  = &~\EEX{}{ \roundb{\hat{V}\roundb{\mx{1}} - \target}^2}  +\label{eq:first}\\
    &~2~\EEX{}{\roundb{\hat{V}\roundb{\mx{1}} - \target}\roundb{\target - \TrV}} + \label{eq:second}\\
    &~\EEX{}{\roundb{\target - \TrV}^2}\label{eq:third}
\end{align}

We aim to study the minimizer of this term.
The first term (\autoref{eq:first}) is the regular bootstrapped Bellman residual with a target $V'$.
The third term (\autoref{eq:third}) is independent of $\hat{V}$, so we can drop it when analyzing the minimization problem.

The second term (\autoref{eq:second}) simplifies to
\begin{align*}
    \EEX{}{\hat{V}\roundb{\mx{1}}\roundb{\target - \TrV}}
\end{align*}
as the remainder is independent of $\hat{V}$ again.

This remaining term however is not independent of $\hat{V}$ and not equal to $0$ either.
Instead, it decomposes into a variance-like term, using the conditional independence of $\mx{1}$ and $\px{1}$ given $\px{0}$:
\begin{align*}
    &~\EEX{}{ \hat{V}\roundb{\mx{1}}\roundb{\target - \TrV}}\\
    =&~\EEX{}{ \hat{V}\roundb{\mx{1}}\target} - \EEX{}{\hat{V}\roundb{\mx{1}}\TrV}\\
    =&~\EEX{}{ \hat{V}\roundb{\mx{1}}\target} - \EEX{}{\hat{V}\roundb{\mx{1}}} \EEX{}{\TrV}.
\end{align*}

% To highlight that this term does not vanish even at the minimum of the Bellman residual, we can substitute $V' = \hat{V} = V$ and obtain
% \begin{align*}
%     &~\EEX{}{ \hat{V}\roundb{\mx{1}}\target} - \EEX{}{\hat{V}\roundb{\mx{1}}\TrV}\\
%     &~\EEX{}{ V\roundb{\mx{1}}^2} - \EEX{}{V\roundb{\mx{1}}}^2 = \text{Var}\squareb{V\roundb{\mx{1}}}.
% \end{align*}

Combining this with \autoref{eq:first}, we obtain
%
\begin{align*}
    &\EEX{}{\hat{\mathcal{L}}^1_\text{MuZero}(p, \hat{V}; \mathcal{D}, V')}\nonumber\\
  = &~\EEX{}{ \roundb{\hat{V}\roundb{\mx{1}} - \roundb{\mathcal{T}V'}\roundb{\mx{1}}}^2} + \\&~\EEX{}{ \hat{V}\roundb{\mx{1}}\target} - \EEX{}{\hat{V}\roundb{\mx{1}}} \EEX{}{\TrV}.
\end{align*}

The first summand is the Bellman residual, which is minimized by $\mathcal{T}V'$, which is in the function class by assumption.
However, by Lemma 2, $\mathcal{T}{V'}$ does not minimize the whole loss term under the conditions (open function class, non-constant value functions, and non-degenerate transition kernel).

\hfill \ensuremath{\Box}

\paragraph{Discussion:}

The proof uses Bellman completeness, which is generally a strong assumption.
However, this is only used to simplify showing the contradiction at the end, removing it does not remove the problems with the loss.
The proof of Lemma 2 can be adapted to the case where $f(x)$ minimizes the difference to $g(x)$, instead of using $g(x)$ as the global minimum, but some further technical assumptions about the existence of minimizers and boundary conditions are needed. 
The purpose here is to show that even with very favorable assumptions such as Bellman completeness, the MuZero value function learning algorithm will not converge to an expected solution.

Similarly, the condition of openness of the function class simply ensures that there exists a function "nearby" that minimizes the loss better.
This is mostly to remove edge cases, such as the case where the function class exactly contains the correct solution.
Such cases, while mathematically valid, are uninteresting from the perspective of learning functions with flexible function approximations.

We only show the proof for the single step version and remove action dependence to remove notational clutter, the action-dependent and multi-step versions follow naturally.

\subsection{Comparison of IterVAML and MuZero for model learning}
\label{app:comp_itervaml_muzero}

\renewcommand{\TrVp}{\squareb{r\roundb{x^{(1)}_i} + \gamma V'\roundb{x^{(2)}_{i}} }}
\newcommand{\TrVpE}{\EEX{x_i^{(1)}, x_i^{(2)} \sim p}{\TrVp}}
\newcommand{\operatorVp}{\mathbb{E}\squareb{\mathcal{T}V'\roundb{\px{1}}}}

If MuZero is instead used to only update the model $\hat{p}$, we obtain a similarity between MuZero and IterVAML. 
This result is similar to the one presented in \cite{grimm2021proper}, so we only present it for completeness sake, and not claim it as a fully novel contribution of our paper.
While \cite{grimm2021proper} show that the whole MuZero loss is an upper bound on an IterVAML-like term, we highlight the exact term the model learning component minimizes.
However, we think it is still a useful derivation as it highlights some of the intuitive similarities and differences between IterVAML and MuZero and shows that they exist as algorithms on a spectrum spanned by different estimates of the target value function.

We will choose a slightly different expansion than before, using $\TrVpE = \operatorVp$

\begin{align}
    &\EEX{}{\hat{\mathcal{L}}^1_\text{MuZero}(\hat{p}, V; \mathcal{D}, V')}\nonumber\\ 
  = &~\EEX{}{\squareb{ V\roundb{\mx{1}} - \operatorVp + \operatorVp - \TrVp  }^2}\nonumber\\
  = &~\EEX{}{ \roundb{V\roundb{\mx{1}} - \operatorVp}^2}  +\label{eq:sfirst}\\
    &~2\EEX{}{\roundb{V\roundb{\mx{1}} - \operatorVp}\roundb{\operatorVp - \TrVp}} + \label{eq:ssecond}\\
    &~\EEX{}{\roundb{\operatorVp - \TrVp}^2}\label{eq:sthird}.
\end{align}


The first summand (\autoref{eq:ssecond}) is similar to the IterVAML loss, instead of using the next state's value function, the one-step bootstrap estimate of the Bellman operator is used.

The third term (\autoref{eq:ssecond}) is independent of $\hat{p}$ and can therefore be dropped. The second term decomposes into two terms again,
\begin{align*}
    &~\EEX{}{\roundb{V\roundb{\mx{1}} - \operatorVp}\roundb{\operatorVp - \TrVp}}\\
    =&~\EEX{}{\roundb{V\roundb{\mx{1}}}\roundb{\operatorVp - \TrVp}} - \\
    &~\EEX{}{\operatorVp\roundb{\operatorVp - \TrVp}}.
\end{align*}

The first summand is equal to $0$, due to the conditional independence of $\mx{1}$ and $\px{1}$,
\begin{align*}
    &~\EEX{}{\roundb{V\roundb{\mx{1}}}\roundb{\operatorVp - \TrVp}}\\
    =&~\EEX{}{V\roundb{\mx{1}}} \underbrace{\roundb{\EEX{}{\operatorVp} - \EEX{}{\TrVp}}}_{= 0} = 0.
\end{align*}

The second remaining summand is independent of $\hat{p}$ and therefore irrelevant to the minimization problem.

Therefore, the MuZero model learning loss minimizes

\begin{align*}
    \EEX{}{\hat{\mathcal{L}}^1_\text{MuZero}(\hat{p}, V; \mathcal{D}, V')} = \EEX{}{ \roundb{V\roundb{\mx{1}} - \operatorVp}^2}.
\end{align*}

In conclusion, the MuZero loss optimizes a closely related function to the IterVAML loss when used solely to update the model.
There are three differences:
First, the bootstrap value function estimator is used instead of the value function as the target value.
Second, the current value function estimate is used for the model sample and the target network (if used) is applied for the bootstrap estimate. If the target network is equal to the value function estimate, this difference disappears.
Finally, the loss does not contain the inner expectation around the model value function.
This can easily be added to the loss and its omission in MuZero is unsurprising, as the loss was designed for deterministic environments and models.

The similarity between the losses suggests a potential family of decision-aware algorithms with different bias-variance characteristics, of which MuZero and IterVAML can be seen as two instances.
It is also interesting to note that even without updating the value function, the MuZero loss performs an implicit minimization of the difference between the current value estimate and the Bellman operator via the model prediction.
This is an avenue for further research, as it might explain some of the empirical success of the method disentangled from the value function update.

\subsection{Propositions from \cite{bertsekasshreve1978}}
\label{sec:bertsekas}

For convenience, we quote some results from~\cite{bertsekasshreve1978}. These are used in the proof of Lemma~\ref{lem:deterministic_representation_lemma}.

\begin{proposition}[Proposition~7.30 of~\cite{bertsekasshreve1978}]
\label{prop:730}
    Let $\mathcal{X}$ and $\mathcal{Y}$ be separable metrizable spaces and let $q(\mathrm{d}y|x)$ be a continuous stochastic kernel on $\mathcal{Y}$ given $\mathcal{X}$. If $f\in\mathcal{C}(\mathcal{X}\times \mathcal{Y})$, the function $\lambda: \mathcal{X} \rightarrow \mathbb{R}$ defined by
    \[
        \lambda(x) = \int f(x,y) q(\mathrm{d}y|x)
    \]
    is continuous.
\end{proposition}

\newcommand{\proj}{\text{proj}}
\begin{proposition}[Proposition~7.33 of~\cite{bertsekasshreve1978}]
\label{prop:733}
    Let $\mathcal{X}$ be a metrizable space, $\mathcal{Y}$ a compact metrizable space, $\mathcal{D}$ a closed subset of $\mathcal{X}\times\mathcal{Y}$, $\mathcal{D}_x = \{y | (x,y) \in \mathcal{D} \}$, and let $f:\mathcal{D}\rightarrow \mathbb{R}^*$ be lower semicontinuous.
    Let $f^*:\proj_\mathcal{X}(\mathcal{D}) \rightarrow \mathbb{R}^*$ be given by \[
    f^*(x) = \min_{y \in \mathcal{D}_x} f(x,y).
    \]
    Then $\proj_\mathcal{X}(\mathcal{D})$ is closed in $\mathcal{X}$, $f^*$ is lower semicontinuous, and there exists a Borel-measurable function $\phi: \proj_\mathcal{X}(\mathcal{D}) \rightarrow \mathcal{Y}$ such that $\text{range}(\phi) \subset \mathcal{D}$ and \[
    f\squareb{x, \phi(x)} = f^*(x), \quad \forall x \in \proj_\mathcal{X}(\mathcal{D}).
    \]
\end{proposition}

In our proof, we construct $f^*$ as the minimum of an IterVAML style loss and equate $\phi$ with the function we call $f$ in our proof.
The change in notation is chosen to reflect the modern notation in MBRL -- in the textbook, older notation is used.

\subsection{Bias due to the stabilizing loss}
\label{app:bias_latent_model}

As highlighted in \autoref{sec:representations}, the addition of a stabilizing loss is necessary to achieve good performance with any of the loss functions.
With deterministic models, the combination $\hat{\mathcal{L}}_\text{IterVAML} + \hat{\mathcal{L}}^n_\text{latent}$ is stable, but the conditions for recovering the optimal model are not met anymore.
This is due to the fact that $\argmin_{\hat{f}} \EEX{}{\hat{\mathcal{L}_\text{latent}}(\hat{f},\phi; \mathcal{D})} = \EEX{}{\phi(x'}$, but in general $\EEX{}{V(\phi(x'))} \neq V\roundb{\EEX{}{\phi(x')}}$.
While another stabilization technique could be found that does not have this problem, we leave this for future work.

\subsection{Multi-step IterVAML}
\label{app:multi-step-itervaml}

In \autoref{sec:model_losses} the multi-step extension of IterVAML is introduced.
As MVE and SVG require multi-step rollouts to be effective, it became apparent that simply forcing the one-step prediction of the value function to be correct is insufficient to obtain good performance.
We therefore extended the loss into a multi-step variant.

Using linear algebra notation for simplicity, the single-step IterVAML loss enforces
\begin{align*}
    \min \left| \langle V, P(x,a)-\hat{P}(x,a)\rangle\right|^2
\end{align*}

The $n$-step variant then seeks to enforce a minimum between $n$ applications of the respective transition operators 
\begin{align*}
    \min \left| \langle V, P^n(x,a)-\hat{P}^n(x,a)\rangle\right|^2
\end{align*}

The sample-based variant is a proper regression target, as $V(x^{(j)}$ is an unbiased sample from $P^n(x,a)$.
It is easy to show following the same techniques as used in the proofs of propositions 1 and 2 that the sample-based version indeed minimizes the IterVAML loss in expectation.

Finally, we simply sum over intermediate $n$-step versions which results in the network being forced to learn a compromise between single-step and multi-step fidelity to the value function.

\section{$\lambda$-Reinforcement Learning Algorithms}
\label{app:lambda-table}

\begin{table}[t]
    \centering
    {\footnotesize
    \begin{tabular}{c|c|c|c|c|c|c}
         & Model loss & Value est. & Policy est. & Actor policy & DAML & Latent \\\hline\hline
        $\lambda$-IterVAML & BYOL & MVE & SVG & direct & \checkmark & \checkmark\\
        $\lambda$-MuZero & BYOL &  MuZero & SVG & direct & \checkmark & \checkmark \\\hline
        MuZero \parencite{schrittwieser2020mastering} & - & MuZero & - & MCTS & \checkmark & \checkmark \\
        Eff.-MuZero \parencite{ye2021mastering} & - & MuZero & - & MCTS & \checkmark & \checkmark \\
        ALM \parencite{ghugare2023simplifying} & ALM-ELBO & m.-free & ALM-SVG & direct & \checkmark & \checkmark \\
        TD-MPC \parencite{tdmpc} & BYOL & m.-free & DDPG & MPC & \checkmark & \checkmark \\\hline
        MBPO \parencite{mbpo} & MLE & SAC & SAC & direct & - & - \\
        Dreamer \parencite{Hafner2020Dream} & ELBO & MVE & SVG & direct & - & \checkmark \\
        IterVAML \parencite{itervaml} & - & Dyna & - & direct & \checkmark & -\\
        VaGraM \parencite{voelcker2022value} & weigthed MLE & SAC & SAC & direct & \checkmark (?) & -
    \end{tabular}
    }
    \caption{An overview of different model-based RL algorithms and how they fit into the $\lambda$-AC family. The first two are the empirical algorithms tested in this work. The next section contains work that falls well into the $\lambda$-AC family as described in this paper. The similarities between these highlight that further algorithms are easily constructed, i.e. ALM \parencite{ghugare2023simplifying} combined with MPC. The final section contains both popular algorithms and closely related work which inform the classification, but are not part of it since they are either not latent or not decision-aware.}
    \label{tab:lambda-overview}
\end{table}

We introduce the idea of the $\lambda$-AC framework in \autoref{sec:lambda-intro}.
The characterization of the family is fairly broad, and it contains more algorithms than those directly discussed in this paper.
An overview of related algorithms is presented in \autoref{tab:lambda-overview}.
However, we found it useful to establish that many recently proposed algorithms are closely related and contain components that can be combined freely with one another.
While the community treats, i.e. MuZero, as a fixed, well-defined algorithm, it might be more useful to treat it as a certain set of implementation and design decisions under an umbrella of related algorithms.
Given the findings of this paper for example, it is feasible to evaluate a MuZero alternative which simply replaces the value function learning algorithm with MVE, which should be more robust in stochastic environments.

A full benchmarking and comparison effort is out of scope for this work.
However, we believe that a more integrative and holistic view over the many related algorithms in this family is useful for the community, which is why we present it here.

\section{Motivating examples}
\subsection{Motivating example for observation spaces}
\label{app:observation_motivation}

To further motivate our focus on observation models, consider the common problem of representing rotational angles.
As these are continuous values, discretized one-hot representations may introduce errors that may harm efficient control.
When choosing a continuous representation, designers are faced with the choice between representing the angle as $\omega\in[-\pi,\pi)$ (the exclusion of the right endpoint is arbitrary), or by a decomposition into $[\sin(\omega),\cos(\omega)]\in [0,1]^2$.
The choice of representation has measurable impact on the ease of learning: the former is 1D instead of 2D, which can reduce the size of networks needed when dealing with many angles. However a complication with the first representation is it is discontinuous at the right endpoint, in the sense that $\lim_{x\to \pi}x = -\pi$ (this is generally due to the structure of $\mathbb{R}/2\pi\mathbb{Z}$). This creates a peculiar continuity condition for functions on this representation space: for a function $f$ to be continuous, it must be continuous and also satisfy the boundary condition $\lim_{x\to \pi}f(x) = f(-\pi)$. Without explicitly enforcing this, the majority of functions learned on this domain (such as estimated value functions) will be discontinous, leading to additional difficulties in the learning process. On the other hand, this issue does not exist for the second representation, potentially making learning much easier.

The important question is that of \emph{similarity} and \emph{continuity}: in our pendulum example, the more compact representation breaks the intuitive notion that states that behave similariy should be mapped to close representations.

\subsection{Distractions}
\label{app:distraction_motivation}

The optimality of features can be measured in how close the projection of the true value function onto their span is to the ground truth, i.e. in the $L_2$ norm.
This raises the question under what conditions the top $k$ eigenvectors or singular vectors would not span the value function well.
For this, we turn to our notion of an MDP with distractions.

\begin{restatable}[Suboptimality of top $k$ eigenspaces with distractions]{proposition}{topkDistracted} \label{prop:SubptimalTopKProduct} Assume an MDP with distraction composed of two independent processes $\gM$ and $\gN$ according to \cref{def:distracting}.
Let $v_1,\dots,v_n$ and $u_1,\dots,u_m$ be the eigenvectors of $\gM$ and $\gN$ respectively, with associated real eigenvalues $\lambda_j$ and $\mu_i$ ordered.
Assume $\forall i < k: \mu_i > \lambda_2$ and $r_\gN = 0$.
Let $U_k$ be an orthonormal basis for the top-k eigenspace of $\gM \otimes \gN$.
Then $\mathrm{span}(U_k) = \mathrm{span}(\{\vone \otimes v_i|\forall i \leq k\})$ and $\mathrm{proj}_{U_k}(r_\gM \otimes \vone) = (\sum_{i=0}^m r_i / n) \vone_{n\cdot m}$.
\end{restatable}

This means that there is a natural notion of a process in which the top-$k$ eigenvectors do not span the reward function well.
In this case, the distracting process has larger eigenvalues than the reward-relevant process.
As the eigenvector basis is composed of Kronecker products of the individual eigenvectors, the top-$k$ eigenspace contains redundant copies of the reward relevant processes eigenvectors.
By \autoref{lem:spectrum_rew_value}, this directly implies suboptimal value function approximation.
Note that our assumptions here are restrictive as we consider a \emph{worst case} distraction for clarity, but the problem emerges whenever the distracting process has several large eigenvalues.


\section{Helpful definitions and lemmata}

This section contains helpful lemmata that are used for our proofs.
Where we took these from existing work, we provide references, otherwise the proofs are our own, although probably also known in the literature.

\subsection{Linear Algebra}
\label{app:linalg}

As before, for matrices $V$ we write $\mathrm{span}(V)$ to denote the span of their column vectors. 

\begin{definition}[Top-$k$ singular vectors]\label{def:topk-single}
Let $P^\pi = U^\top \Sigma V$ be the singular value decomposition, and assume that the diagonal of $\Sigma$ is arranged in decreasing order.
The first $k$ rows of $U^\top$ and the first $k$ columns of $V$ are called the top-k left and right singular vectors, respectively.
\end{definition}

\begin{lemma}[Spectrum of Kronecker product matrix]\label{lem:spectrum_kronecker}
    Consider two non-singular matrices $M$ and $N$. Let $\lambda_i$ be the eigenvalues of $M$ and $\mu_j$ be the eigenvalues of $N$, with eigenvectors $u_i$ and $v_j$ respectively. Then the eigenvalues of $M\otimes N$ are $\lambda_i\mu_j$ with eigenvectors $u_i \otimes v_j$ respectively.
\end{lemma}

\begin{proof}
For any eigenvector $u_i$ of $M$ and $v_j$ of $N$, we have
    $$\left(M\otimes N\right) \left(u_i \otimes v_j\right) = \left(M u_i\right) \otimes \left(N v_j\right) = \left(\lambda_i u_i\right)\otimes\left(\mu_j v_j\right) = \lambda_i\mu_j \left(u_i \otimes v_j\right).$$

    So $u_i\otimes v_j$ is an eigenvector of $M\otimes N$ with eigenvalue $\lambda_i \mu_j$
\end{proof}

\begin{lemma}[Orthogonalized bases and the Kronecker product]\label{lem:orth_kronecker}
    Let $V \in \vecR{n}{m}$ be a matrix. Let $\mathrm{orth}(V)$ be a matrix of any orthonormal basis vectors for the column vectors of $V$. Then $\mathrm{span}(V\otimes \vone) = \mathrm{span}(\mathrm{orth}(V) \otimes \vone)$. Furthermore $(1/k) \mathrm{orth}(V) \otimes \vone$ is an orthonormal basis of $\mathrm{span}(V \otimes \vone)$.
\end{lemma}

\begin{proof}
    Let $V_i \otimes \vone^k$ be any column vector of $V\otimes \vone^k$. Let $\alpha_1,\dots,\alpha_m$ be coefficients so that $V_i = \sum_{j=1}^m \alpha_j \mathrm{orth}(V)_j$. Then $$\sum_{j=1}^m \alpha_j \left(\mathrm{orth}(V)_j \otimes \vone^k\right) = \left(\sum_{j=1}^m \alpha_j \mathrm{orth}(V)_j\right) \otimes \vone^k = V_i \otimes \vone^k,$$ following the standard associative and distributive properties of the Kronecker product.

    As every vector in the original span can be represented in the orthogonalized span, the two are equivalent.

    Finally note that $$\frac{1}{k}\left(\mathrm{orth}(V) \otimes \vone\right)_i^\top \left(\mathrm{orth}(V) \otimes \vone\right)_i = \frac{1}{k}\sum_{j=1}^k \left(\mathrm{orth}(V)_i^\top \mathrm{orth}(V)_i^\top\right) = 1,\,\text{and}$$ $$\frac{1}{k}\left(\mathrm{orth}(V) \otimes \vone\right)_j^\top \left(\mathrm{orth}(V) \otimes \vone\right)_i = \frac{1}{k}\sum_{t=1}^k \left(\mathrm{orth}(V)_j^\top \mathrm{orth}(V)_i^\top\right) = 0.$$
\end{proof}

\begin{lemma}[Reduced rank regression]\label{lem:rrr}
    Let $C,D \in \vecR{n}{n}$, $A\in\vecR{n}{k}$, and $B\in\vecR{k}{n}$ be full rank matrices with $n \geq k$. Let $A^*, B^* = \min_{A,B} \lVert CAB - DC\rVert^2_F$. Let ${u_1,\dots,u_k}$ and ${v_1\dots,v_k}$ be the top-k singular vectors of $C^{-1}DC$ according to \autoref{def:topk-single}. Then $\mathrm{span}(A^*) = \mathrm{span}({u_1,\dots,u_k})$ and $\mathrm{span}(B) = \mathrm{span}({v_1,\dots,v_k})$.
\end{lemma}

This is a reduced rank regression problem \parencite{izenman1975reduced} or low-rank matrix approximation problem. 

The unconstrained solution to the problem is given by $\hat{A}\hat{B} = C^{-1}DC$. From the Eckhart-Young theorem, we know that the optimal low-rank approximation to $C^{-1}DC$ is given by the top-k singular vectors.
Therefore $A$ and $B$ span top-$k$ left and right singular vectors respectively.
For a more extensive proof, please refer to \cite{izenman1975reduced}.

\subsection{Stochastic matrices}
\begin{lemma}[Spectrum of a resolvent matrix]\label{lem:spectrum_resolvent}
    Let $A$ be an invertible matrix with unique real eigenvalues and $-1 \leq \lambda_{\min} \leq \lambda_{\max} \leq 1 $. Let $\gamma\in(0,1)$.
    The matrices $A$ and $(I - \gamma A)^{-1}$ have the same eigenvectors and the ordering of the corresponding eigenvalues remains the same.
\end{lemma}

\begin{proof}
    Let $e$ be an eigenvector of $A$ and $\lambda$ the corresponding eigenvalue. Then $$(I - \gamma A)^{-1}e = \sum_{i=0}^n \gamma^n A^ne = \sum_{i=0}^n (\gamma\lambda)^n e = \frac{1}{1 - \gamma\lambda}e.$$ As $(1-\gamma\lambda)^{-1}$ is a monotonic function for $-1 \leq \lambda \leq 1$ and $\gamma\in[0,1]$, the ordering of the eigenvalues remains the same.
\end{proof}

\begin{lemma}[Basis equivalence of linear reward and value function]\label{lem:spectrum_rew_value}
    Let $r$ be the vector representation of the reward function and $V$ of the value function respectively for an MDP with fixed policy and transition matrix $P^\pi$.
    Let $U = \{u_1,\dots,u_n\}$ be the set of eigenvectors of $P^\pi$, and let $U^r \subseteq U$ be a minimal set of eigenvectors so that $r \in \mathrm{span}(U^r)$.
    Then $V \in \mathrm{span}(U^r)$.
\end{lemma}
\begin{proof}
    Let $r = \sum_{i=1}^k \alpha_i\, u^r_i$ be the reward representation in the basis $U^r$. Then, by \autoref{lem:spectrum_resolvent} $$V^\pi = (I - \gamma P^\pi)^{-1} r = \sum_{i=1}^k \frac{\alpha}{1 - \lambda_i\gamma}  u^r_i.$$
\end{proof}


\subsection{Ordinary differential equations}

For describing the stability of ODEs, we use the notion of \emph{asymptotic stability}, with the condition $\Re(\lambda_i) < 0$ for all eigenvalues $\lambda_i$ of the Jacobian at a critical point.

\begin{lemma}[Linear reparameterization of an autonomous ODE]
\label{lem:stability}
Let $y' = f(y)$ be an autonomous ordinary differential equation. Let $y^*$ be any critical point for which $f(y^*) = 0$. Let furthermore $x' = f(A x)$ be a reparameterized autonomous ODE for any invertible matrix $A$.
Then the $x^*$ is a critical point with $f(Ax) = 0$ iff $x^* = A^{-1}y^*$.
Furthermore, the eigenvalues of the Jacobian of $y'$ at $y^*$ are equal to the eigenvalues of the Jacobian of $x'$ at $x^* = A^{-1}y^*$.
\end{lemma}

\begin{proof}
The direction $x = A^{-1}y^* \Rightarrow f(Ax) = 0$ is clear by direct evaluation. We now focus on the direction $f(Ax) = 0 \Rightarrow x = A^{-1}y^*$. Assume that $f(Ax)$ is $0$ and $x = A^{-1}y$ for a $y$ which is not a point satisfying $f(y)=0$. But then $0 = f(AA^{-1}y) = f(y) \neq 0$, a contradiction.

For stability, note that
\begin{alignat*}{2}
    &\qquad&\ddt{t} y & = f(y) \\
    \implies&&\ddt{t} x & = \ddt{y} x \ddt{t} y\\
    && &= A^{-\top} f(y)\\
    \implies&& \ddt{x} \ddt{t} x &= A^{-\top} \ddt{x} f(A x)\\
    && &= A^{-\top}  \ddt{y} f(y) \ddt{x} A x\\
    && &= A^{-\top}  \ddt{y} f(y) A^\top.
\end{alignat*}
This shows that the Jacobian $\ddt{x} f(Ax)|_{x=A^{-1}y_0}$ is similar to the Jacobian $\ddt{y} f(y)|_{y=y_0}$, which means their spectra are identical.
\end{proof}


\subsection{MDP representation and TD learning}

\begin{lemma}[Lemma 5 of \cite{tang2022understanding}]\label{prop:TangResult2}
    Suppose $P^\pi$ is real-diagonalizable, and write $u_1,\dots, u_n$ for its eigenvectors. Then any orthonormal matrix $\Phi$ which has the same span as a set of $k$ eigenvectors is a minimizer of $L_{\text{lat}}$.
\end{lemma}


The next three statements are taken from \cite{ghosh2020representations}.  They address the TD loss wrt to the learned weights $\hat{V}$ and fixed $\Phi$ (compare \autoref{sec:background}).
We changed the notation of the statements to fit our notation here, we have denoted the diagonal matrix of the state distribution as $D$ and assume that $D=I$ in \autoref{assumption1}, while \cite{ghosh2020representations} uses $\Xi$. They use $\theta$ for the value function weights while we use $\hat{V}$.
They also uses $\text{Spec}(A)$ to denote the spectrum, the set of all eigenvalues of a matrix $A$.

The notion of stability used in \cite{ghosh2020representations} is that of convergence to the unique fixed point of the projected Bellman update of the linear ODE induced by the $L_\text{TD}$ loss when fixing $\Phi$. In the two-timescale scenario considered in this paper, this corresponds to the ``inner'' ODE over $\hat{V}$.

\begin{lemma}[Proposition 3.1 of \cite{ghosh2020representations}]\label{prop:gosh1}
    TD(0) is stable if and only if the eigenvalues of the implied iteration matrix $A_\Phi = \Phi^\top D (I - \gamma P^\pi) \Phi$ have positive real components, that is $$\text{Spec}\left(A_\Phi\right) \subset \mathbb{C}_+ \coloneq \{z : \text{Re}(z) > 0 \}.$$
    We say that a particular choice of representation $\Phi$ is stable for $(P^\pi, \gamma, D)$ when $A_\Phi$ satisfies the above condition.
\end{lemma}

\begin{lemma}[Proposition 3.2 of \cite{ghosh2020representations}]\label{prop:gosh2}
    An orthogonal representation $\Phi$ is stable if and only if the real part of the eigenvaluse of the induced transition matrix $\Pi P^\pi \Pi$ where $\Pi = \Phi\Phi^\top$ is bounded above, according to $$\text{Spec}\left(\Pi P^\pi \Pi\right) \subset \{z \in \mathbb{C}: \text{Re}(z) < \frac{1}{\gamma} \}.$$
    In particular, $\Phi$ is stable if $\rho(\Pi P^\pi\Pi) < \frac{1}{\gamma}$.
\end{lemma}

\begin{lemma}[Theorem 4.1 of \cite{ghosh2020representations}]\label{prop:gosh3}
    An orthogonal invariant representation $\Phi$ (meaning $\text{span}(P^\pi \Phi) \subseteq \text{span}(\Phi)$) satisfies $$\text{Spec}\left(\Pi P^\pi \Pi\right) \subseteq \text{Spec}(P^\pi) \cup \{0\}$$ and is therefore stable.
\end{lemma}

As a corollary of their proof we have that 

\begin{lemma}[Corollary of \cite{ghosh2020representations}]\label{gosh:corr}
    Let $\Phi$ be an orthogonal (but not necessarily square) invariant representation of $P^\pi$. Then the spectral radius $\rho(\Phi^\top P^\pi \Phi) \leq 1.$ 
\end{lemma}

The proof follows directly from \autoref{prop:gosh3} by the cyclicality of the spectrum.
    

The following two results are our own, although closely related results exist in the literature.

\begin{lemma}[Lossless approximation of $V$]\label{lem:lossless_approx}
    Let $\Phi$ be an orthonormal basis of an invariant subspace of $P^\pi$ and let \autoref{ass:low_rank} hold. Let $V = (I - \gamma P^\pi)^{-1} r$ be the value function of $P^\pi$ and $r^\pi$. Then $$\Phi (I - \gamma \Phi^\top P^\pi \Phi)^{-1} \Phi^\top r^\pi = V.$$
\end{lemma}
\begin{proof}
    By \autoref{ass:low_rank} we can find a matrix $A$ so that $r^\pi = \Phi A$ and $\Phi\Phi^\top r^\pi = r^\pi$, and by the invariant subspace assumption, we can find a matrix $B$ so that $P^\pi \Phi = \Phi B$.
    Writing the inverted matrix as an infinite sum, which is valid as the spectrum of $\Phi^\top P^\pi \Phi$ is bounded by 1 following from \autoref{gosh:corr} and Carl Neumann's theorem over power series, we obtain
    \begin{align*}
        \Phi (I - \gamma \Phi^\top P^\pi \Phi)^{-1} \Phi r^\pi &= \Phi \sum_{n=0}^\infty \gamma^n (\Phi^\top P^\pi \Phi)^n \Phi r^\pi \\
        &= \Phi \sum_{n=0}^\infty \gamma^n B^n \Phi^\top r^\pi &\quad (P^\pi \Phi = \Phi B)~\text{and}~(\Phi^\top \Phi = I)\\
        &= \sum_{n=0}^\infty \gamma^n \Phi B^n \Phi^\top r^\pi \\
        &= \sum_{n=0}^\infty \gamma^n {P^\pi}^n \Phi\Phi^\top r^\pi &\quad (\Phi B = P^\pi \Phi)~\text{iterated} \\
        &= \sum_{n=0}^\infty \gamma^n {P^\pi}^n r^\pi &\quad (\Phi\Phi^\top r^\pi = r^\pi) \\
        &= V
    \end{align*}
\end{proof}


\begin{lemma}[Critical points of $L_\text{TD}$]\label{prop:td_critical}
Let \autoref{assumption1}, \autoref{assumption3}, and \autoref{ass:low_rank} hold.
    Assume $\Phi^* \in \mathbb{R}^{n \times k}$ is an orthonormal invariant representation for $P^\pi$ in the sense that ${\Phi^*}^\top \Phi^* = I$ and $\text{span}(\Phi^*) = \text{span}(P^\pi \Phi^*)$. Let furthermore  $r^\pi \in \text{span}(\Phi^*)$ and let $V$ be the corresponding value function. Then $\Phi^*$ is a critical point of $L_\text{TD}$ and for the corresponding weights $\hat{V}^*$ we have $\Phi^*\hat{V}^* = V$.
\end{lemma}

\begin{proof}
    By \autoref{prop:gosh1}, \autoref{prop:gosh2}, and \autoref{prop:gosh3} and the stated assumptions the weights $\hat{V}$ converge. Therefore, we can analyze the induced dynamical system with $\hat{V}*$.

    Find $\hat{V}^*$ as the stationary point of $\nabla_{\hat{V}} L_\text{TD}$
    \begin{align*}
        & \nabla_{\hat{V}} \ltwonorm{\Phi^* \hat{V} - \left[r^\pi + \gamma P\Phi^* \hat{V}\right]_\text{sg}}^2\bigg|_{\hat{V} = \hat{V}^*} = {\Phi^*}^\top\left(\Phi^* \hat{V}^* - \left[r^\pi + \gamma P\Phi^* \hat{V}^*\right]\right) = 0 \\
        \Leftrightarrow \quad & {\Phi^*}^\top \Phi^* \hat{V}^* - {\Phi^*}^T r^\pi - \gamma {\Phi^*}^\top  \Phi^* B \hat{V}^* = ({\Phi^*}^\top \Phi^* - \gamma {\Phi^*}^\top P^\pi \Phi^*) \hat{V}^* - {\Phi^*}^\top r^\pi = 0\\
        \Leftrightarrow \quad & \hat{V}^* = ({\Phi^*}^\top \Phi^* - \gamma {\Phi^*}^\top P^\pi \Phi^*)^{-1} {\Phi^*}^\top r^\pi = (I - \gamma {\Phi^*}^\top P^\pi \Phi^*)^{-1} {\Phi^*}^\top r^\pi
    \end{align*}
    The invertibility of $(I - \gamma \Phi^\top P^\pi \Phi) = \Phi^T D (I - \gamma P^\pi) \Phi = A_\Phi$ is guaranteed as all eigenvectors are nonzero.

    We now show that $\Phi^*$ is a stationary point by showing that $$\nabla_\Phi L_\text{TD}|_{\Phi = \Phi^*} = 0.$$ We note that as $\Phi^*$ spans an invariant subspace of $P^\pi$ there exists an invertible matrix $B$ so that $P^\pi \Phi^*  \Phi^* B$. Therefore $(I - \gamma {\Phi^*}^\top P^\pi \Phi^*) = (I - \gamma B)$.

    \begin{align*}
        \nabla_\Phi L_\text{TD}|_{\Phi = \Phi^*} & =  \nabla_{\Phi} \ltwonorm{\Phi \hat{V}^* - \left[r^\pi + \gamma P\Phi \hat{V}^*\right]_\text{sg}}^2\Bigg|_{\Phi = \Phi^*}\\
        & = \left(\Phi \hat{V}^* - r^\pi - \gamma P\Phi {\hat{V}}^*\right) (\hat{V}^*)^\top\Bigg|_{\Phi = \Phi^*} \\
        & = \left(\Phi^* (I - \gamma B) {\hat{V}}^* - r^\pi \right)(\hat{V}^*)^\top \quad\quad \text{(substitute first occurrence of)~$\hat{V}^*$}\\
        & = \left(\Phi^* {\Phi^*}^\top r^\pi - r^\pi \right)({\hat{V}}^*)^\top\\
        & = \underbrace{\left(r^\pi - r^\pi \right)}_{=0}({\hat{V}}^*)^\top = 0
    \end{align*}
    The final line is due to the fact that the columns of $\Phi^*$ are orthonormal, which means that $\Phi^*{\Phi^*}^\top$ is an orthogonal projection onto the span of $\Phi^*$. To verify, note that $$\left(\Phi^*{\Phi^*}^\top\right)^2 = \left(\Phi^* \underbrace{{\Phi^*}^\top \Phi^*}_{=I}{\Phi^*}^\top\right) = \left(\Phi^*{\Phi^*}^\top\right).$$ Furthermore, by \autoref{ass:low_rank}, $r^\pi \in \text{span}(\Phi^*)$, which means $\Phi^*{\Phi^*}^\top r^\pi = r^\pi$ for an orthogonal projection $\Phi^*{\Phi^*}^\top$.
    Moreover, by \autoref{lem:lossless_approx}, $\Phi^* \hat{V}^* = V$, which concludes the proof.
\end{proof}

This proof closely follows related statements by \cite{ghosh2020representations}, \cite{tang2022understanding}, and \cite{lelan2022generalization}. We repeated the argument here for easier legibility with all assumptions necessary for our work.

\section{Proofs of main results}
\label{app:proofs}

\subsection*{Proofs for Section 4}

\BYOLGradientFlow*
\begin{proof}
    At any stationary point the gradient $\frac{d}{dt}\Phi_t$ must be equal to $0$, which from \cref{eq:BYOLTwoTimescale} means that we must have $\left(\Phi_t\left(\Phi^\top_t \Phi\right)^{-1}\Phi_t^\top - I\right)P^\pi\Phi_t \Phi_t^\top {P^\pi}^\top \Phi_t\left(\Phi^\top_t \Phi\right)^{-\top}=0$.

    Assume that the column vectors of $\Phi^*$ spans an invariant subspace of $P^\pi$. This implies that there exists a full rank matrix $A$ so that $P^\pi \Phi^* = \Phi^*A$.
    Then 
    $$\left(\Phi^*\left({\Phi^*}^\top \Phi^*\right)^{-1}{\Phi^*}^\top - I\right)P^\pi\Phi^* F^*=\Big(\Phi^*\underbrace{\left({\Phi^*}^\top \Phi^*\right)^{-1}{\Phi^*}^\top\Phi^*}_{=I} - \Phi^*\Big)A F^*=0.$$

This proves the first part of the proposition.

There are additional critical points of the differential equation, as discussed by \cite{tang2022understanding}.
In the analysis of stability, we first show the case of critical points corresponding to the claim in the proposition.
We then briefly discuss other cases after the proof.

\textbf{Case 1: $\Phi_t$ spans an invariant subspace of $P^\pi$}
Invariant subspaces correspond to subspaces spanned by right eigenvectors of $P^\pi$.

We write $P$ for $P^\pi$ to reduce notational clutter.
Let $e_1,\dots,e_k$ be the eigenvectors corresponding to the $k$ largest eigenvalues of $P$.
Let $\Phi^*$ correspond to any set of $k$ eigenvectors of $P$. Then
\begin{align*}
    \ddt{t}\Phi^* = -\left(\Phi^* F^* - P \Phi^*\right) {F^*}^\top = 0.
\end{align*}

To show that all non top-$k$ eigenspaces are asymptotically unstable critical points of the differential equation defined by the gradient flow of $\Phi$.
To show this, we aim to show that there exists an eigenvector of the Jacobian with an eigenvalue larger than $0$.
For this, we construct the directional derivative at the critical point.
The directional derivative is the Jacobian vector product, which allows us to circumvent the need to work with higher order tensor derivatives.
We then proceed to show that there exists a direction which corresponds to the eigenvector of the Jacobian with positive eigenvalue.
This concludes the proof.
This technique closely follows the one used by \cite{lelan2023bootstrapped}.

Assume $\mathrm{span}\{\Phi^*\} \neq \mathrm{span}\{e_1,\dots,e_k\}$. This implies that there exists at least one eigenvector $e_j \in \{e_1,\dots,e_k\}$ and $e_j \notin \mathrm{span}\{\Phi^*\}$, with corresponding eigenvalues $\lambda_j$.

Let $D_\Delta$ be the directional derivative of $\ddt{t} \Phi |_{\Phi=\Phi^*}$ in the direction $\Delta$.
We construct the directional derivative using the product rule (terms colored for ease of reading),
\begin{align*}
D_\Delta \ddt{t} \Phi |_{\Phi=\Phi^*} = & -D_\Delta \left(\left(\Phi F^* - P \Phi\right) {F^*}^\top\right)|_{\Phi=\Phi^*}\\
     = & -{\color{uoftred}D_\Delta \left(\Phi F^* - P \Phi\right)|_{\Phi=\Phi^*}} {F^*}^\top - \left(\Phi^* F^* - P \Phi^*\right) {\color{uoftmagenta}D_\Delta {F^*}|_{\Phi=\Phi^*}}^\top.
\end{align*}

For the directional derivative, we only consider directions that are orthogonal to $\Phi^*$, so ${\Phi^*}^\top\Delta = 0$. Then $\underbrace{P{\Phi^*} = {\Phi^*} A}_{\text{subspace condition}} \implies \Delta^\top P \Phi^* = 0.$
For the derivative with regard to $F^*$, we have
\begin{align*}
    {\color{uoftmagenta}D_\Delta F^*|_{\Phi = \Phi^*}} =& D_\Delta \left(\Phi^\top \Phi\right)^{-1}\Phi^\top P \Phi\nonumber\\
    =& \left(D_\Delta \left(\Phi^\top \Phi\right)^{-1} \right) {\Phi^*}^\top P {\Phi^*} + \left({\Phi^*}^\top {\Phi^*}\right)^{-1} \left(D_\Delta \Phi^\top P \Phi  \right)\nonumber\\
    =& -\underbrace{(\Delta^\top \Phi^* + {\Phi^*}^\top \Delta)}_{=0}\left({\Phi^*}^\top {\Phi^*}\right)^{-2}{\Phi^*}^\top P {\Phi^*} + \left({\Phi^*}^\top {\Phi^*}\right)^{-1}\big(\underbrace{\Delta^\top P {\Phi^*}}_{=0} + {\Phi^*}^\top P \Delta\big)\nonumber\\
    =& \left({\Phi^*}^\top {\Phi^*}\right)^{-1}\left({\Phi^*}^\top P \Delta\right). \label{eq2}
\end{align*}
Therefore, the first term in the second line is dropped, as well as the first term of the final summand.

Note that $F^* = \left({\Phi^*}^\top{\Phi^*}\right)^{-1} {\Phi^*}^\top P^\pi {\Phi^*} = \underbrace{\left({\Phi^*}^\top{\Phi^*}\right)^{-1} {\Phi^*}^\top {\Phi^*}}_{=I}\, \mathrm{diag}(\Lambda_i) = \mathrm{diag}(\Lambda_i)$, where $\Lambda_i$ is the set of eigenvalues corresponding to the eigenvectors in $\Phi^*$ and $\mathrm{diag}(\Lambda_i)$ is the diagonal matrix of eigenvalues corresponding to those eigenvectors spanned by $\Phi^*$.

This allows us to compute the remaining derivative,
\begin{align*}
    &{\color{uoftred}D_\Delta \left(\Phi F^* - P \Phi\right)|_{\Phi=\Phi^*}}\, \mathrm{diag}(\Lambda_i)\\
    = & \left(\Delta\, \mathrm{diag}(\Lambda_i) + \Phi^*{\color{uoftmagenta} D_\Delta F^*}  - P\Delta \right)\,\mathrm{diag}(\Lambda_i)\\
    = & \left(\Delta\, \mathrm{diag}(\Lambda_i) + \Phi^*\left({\Phi^*}^\top {\Phi^*}\right)^{-1}{\Phi^*}^\top P \Delta  - P\Delta \right)\,\mathrm{diag}(\Lambda_i),
\end{align*}
where we use the fact that $D_\Delta P\Phi^* = P D_\Delta \Phi^* = P \Delta$.

Finally, as $\Phi^*F^* = \Phi^* \mathrm{diag}(\Lambda_i) = P^\pi \Phi^*$, we obtain
\begin{align*}
 D_\Delta \ddt{t} \Phi |_{\Phi=\Phi^*} =   -\left(\Delta\, \mathrm{diag}(\Lambda_i) + {\Phi^*}\left({\Phi^*}^\top \Phi^*\right)^{-1}{\Phi^*}^\top P \Delta  - P\Delta \right)\mathrm{diag}(\Lambda_i) - \underbrace{\left(\Phi^* F^* - P \Phi^*\right)}_{=0\,\text{as shown}}\left({\Phi^*}^\top P \Delta\right)^\top.
\end{align*}


By the definition of the directional derivative as the Jacobian-vector product, we can now assert
\begin{align*}
    \left(\ddt{\Phi} \ddt{t}\Phi |_{\Phi=\Phi^*}\right) \Delta = - \left(\Delta \mathrm{diag}(\Lambda_i) + \left(\Phi^*\left({\Phi^*}^\top {\Phi^*}\right)^{-1}{\Phi^*}^\top - I\right) P \Delta\right)\mathrm{diag}(\Lambda_i).
\end{align*}

What remains to be shown is that there exist a direction which corresponds to a positive eigenvalue of the Jacobian of the dynamics.

Choose $\Delta = v_j u^\top$. Let $v_j$ be an eigenvector not in the span of $\Phi^*$ but in the top-k eigenvectors. Let  $\lambda_j$ be the corresponding eigenvalue. By our assumption before, there exist at least one eigenvalue $\lambda_i \in \Lambda_i$ so that $\lambda_j > \lambda_i$.

Note that $\left(\Phi^*\left({\Phi^*}^\top {\Phi^*}\right)^{-1}{\Phi^*}^\top - I\right) P v_j = \Big(\Phi\left({\Phi^*}^\top {\Phi^*}\right)^{-1}\underbrace{{\Phi^*}^\top v_j}_{0\, \text{by construction}} - I v_j\Big) \lambda_j = -\lambda_j v_j$ and therefore $\left(\Phi^*\left({\Phi^*}^\top \Phi^*\right)^{-1}{\Phi^*}^\top - I\right) P \Delta = -\Delta \lambda_j I.$

To simplify notation, we will write $\Lambda_i$ for $\mathrm{diag}(\Lambda_i)$ from now on as there is no risk of confusion.

\begin{align*}
    -\left(\Delta \Lambda_i + \left(\Phi^*\left({\Phi^*}^\top \Phi^*\right)^{-1}{\Phi^*}^\top - I\right) P \Delta\right)\Lambda_i = &- \Delta \left(\Lambda_i - \lambda_j I\right) \Lambda_i\\
    = &-\Delta \left(\Lambda_i^2 - \lambda_j \Lambda_i\right) \\
    = &\Delta \left(\lambda_j \Lambda_i - \Lambda_i^2\right).
\end{align*}

We can now choose $u$ so that it is any eigenvector of $\left(\lambda_j \Lambda_i - \Lambda_i^2\right)$. As this is a diagonal matrix, it is easy to see that if $\lambda_j > \lambda_i$ for any $\lambda_i$, the matrix will have a positive eigenvalue, meaning there exists a direction in which the critical point is unstable.

\end{proof}


\textbf{Case 2: Non-invariant subspace cases:} Not all critical points lie in invariant subspaces.
One such an alternative critical point is the case of $\Phi^\top P \Phi = 0$, and more generally, for each set of column vectors $\phi_i$ in $\Phi$, they needs to either be mapped to an invariant or an orthogonal subspace by $P^\pi$ to be stable.
In the orthogonal case, the Jacobian at the critical point becomes 0, meaning no conclusion about stability can be drawn from this analysis.

We leave further analysis of non invariant subspace critical points open for future work.
We do however conjecture that the non invariant subspace critical points are also saddle-points or unstable solutions of the ODE, following the experimental analysis by \cite{tang2022understanding}.

\textbf{Negative eigenvalues:} In case the matrix has negative eigenvalues, the stability conditions in the final step of the proof change. The matrix $\lambda_j\Lambda_i - \Lambda_i^2$ will not have negative eigenvalues if $\lambda_i$ is negative but $\lambda_j$ is positive. The ranking of stable points follows this slightly un-intuitive ordering: all negative eigenvalues sorted by absolute value followed by all positive eigenvalues sorted by absolute value.

\ReconstructionStationaryPoints*


\begin{proof}
We first show that under the two timescale scenario, $F$ is stationary and therefore does not change the span of the critical points.

Due to the assumption of the two-timescale scenario, we compute $\Psi^*$ by solving the linear regression problem,
\begin{align*}
    &&\ddt{\Psi^*} \lVert \Phi F\Psi^* - P \rVert_F^2 &= F^\top {\Phi}^\top ({\Phi}F\Psi^* - P)\\
    &&0 &=  F^\top {\Phi}^\top ({\Phi}F\Psi^* - P)\\
    \Leftrightarrow && B^* &= \left(F^\top {\Phi}^\top {\Phi}F\right)^{-1} F^\top \Phi^\top P = F^{-1}\left(\Phi^\top \Phi\right)^{-1} \Phi^\top P.
\end{align*}

Plugging this solution back into the original equation, 
\begin{align}
    \lVert \Phi F\Psi^* - P \rVert_F^2 &= \lVert \Phi FF^{-1}\left(\Phi^\top \Phi\right)^{-1} \Phi^\top P - P \rVert_F^2\\
    &= \lVert \Phi \left(\Phi^\top \Phi\right)^{-1} \Phi^\top P - P \rVert_F^2,
\end{align}
we notice that $F$ cancels.
Therefore, the optimality conditions for $A$ follow from the Eckart-Young theorem, as presented in \autoref{lem:rrr}
\end{proof}

\ReparameterizationInvariance*

\begin{proof}
We first write out all losses with the observation matrix $\gO$. The reward function is assumed to not change under the introduction of $\gO$, therefore we do not multiply $\gO$ to $x^\top r$.

\begin{align*}
    L_{\text{rec}}(\Phi,F,\Psi) =& \EEX{x \sim \mathcal{D}}{\ltwonorm{x^\top \gO \Phi F \Psi - x^\top {P^\pi}\gO}^2} = {\ltwonorm{\gO \Phi F \Psi - {P^\pi}\gO}^2}, \\
    L_{\text{lat}}(\Phi,F) =& \EEX{x \sim \mathcal{D}}{\ltwonorm{x^\top \gO \Phi F - \left[x^\top {P^\pi} \gO \Phi\right]_{\mathrm{sg}}}^2} = \ltwonorm{\gO \Phi F - \left[{P^\pi} \gO \Phi\right]_{\mathrm{sg}}}^2 \\
    L_{\text{td}}(\Phi,F) =& \EEX{x \sim \mathcal{D}}{\ltwonorm{x^\top\gO\Phi \hat{V} - \left[x^\top \left(r + \gamma P^\pi \gO\Phi \hat{V}\right)\right]_{\mathrm{sg}}}^2} = {\ltwonorm{\gO\Phi \hat{V} - \left[\left(r + \gamma P^\pi \gO\Phi \hat{V}\right)\right]_{\mathrm{sg}}}^2}.
\end{align*}

Note that in the cases of $L_{\text{lat}}$ and $L_\text{TD}$, all occurrences of $\gO$ are multiplied by $\Phi$.
Therefore the corresponding gradient flows are reparameterizations as defined in \autoref{lem:stability}, and the proof follows directly.
\end{proof}

\ReparameterizationInvarianceObs*

\begin{proof}
    
As before, note that $L_\mathrm{rec}^\gO$ is of the form $\lVert \gO AXB - P\gO \rVert_F^2$, with $A \in \vecR{n}{k}$, $X \in \vecR{k}{k}$, and $B \in \vecR{k}{n}$.

Due to the assumption of the two-timescale scenario, we compute $\Psi^*$ by solving the linear regression problem,
\begin{align*}
    &&\ddt{\psi} \lVert \gO \Phi F \Psi - P \gO \rVert_F^2 &= F^\top \Phi^\top \gO^\top (\gO \Phi F \Psi - P)\\
    &&0 &=  F^\top \Phi^\top \gO^\top (\gO \Phi F \Psi^* - P) = 0 \\
    \Leftrightarrow &&
    \Psi^* &= \left(F^\top \Phi^\top \gO^\top \gO \Phi F\right)^{-1} F^\top \Phi^\top \gO^\top P.
\end{align*}

Substituting into $L_\mathrm{rec}^\gO$, we obtain
\begin{align*}
    \lVert \gO \Phi F \Psi- P \gO \rVert_F^2 & = \lVert \gO \Phi F \left(F^\top \Phi^\top \gO^\top \gO \Phi F\right)^{-1} F^\top \Phi^\top \gO^\top P - P\gO \rVert_F^2 \\
    & = \lVert \gO \Phi \left( \Phi^\top \gO^\top \gO \Phi\right)^{-1} \Phi^\top \gO^\top P - P\gO \rVert_F^2,
\end{align*}
which again implies that $F$ is stationary.

We note that $\lVert \gO \Phi\Psi - P\gO\rVert^2_F$ is the reduced rank regression problem solved in \autoref{lem:rrr} which solution is given by the top-k left and right singular vectors of $\gO^{-1} P \gO$.
\end{proof}

\subsection*{Proofs for Section 5}


\BYOLCombined*
\begin{proof}
        By \autoref{ass:low_rank} there exists a set of $k$ vectors $\phi_1,\dots,\phi_k$ such that $r^\pi \in \text{span}(\phi_1,\dots,\phi_k)$ and $\phi_1,\dots,\phi_k$ span an invariant subspace of $P^\pi$.
        We can choose this set of vectors to orthonormal, e.g. by applying the Gram Schmidt procedure to the eigenvectors $({w_i}_1,\dots{w_i}_m)$. 
        By \autoref{prop:TangResult2} the matrix $\Phi\in \mathbb{R}^{n\times k}$ whose columns are $\phi_1,\dots,\phi_k$ is a critical point for $L_\text{lat}$ and spans an invariant subspace of $P^\pi$.
        In addition, by \autoref{lem:spectrum_rew_value}, $\Phi$ forms a complete basis for the value function $V^\pi$.
        By \autoref{prop:td_critical}, $\Phi$ is also a critical point of $L_\text{TD}$, with $0$ value function approximation error. 
        As $\Phi$ is a critical point for both $L_\text{TD}$ and $L_\text{Lat}$, it is a critical point of $L_\text{TD} + L_\text{Lat}$.

\end{proof}

\ReconsCombined*
\begin{proof}
    Following from \autoref{ass:low_rank} and \cref{lem:spectrum_rew_value}, we have that any critical point $\Phi^*$ of $L_\text{td}$ with perfect value function approximation fulfils $\mathrm{span}(r^\pi)\subseteq \mathrm{span}(\Phi^*)$, as without this condition, $\Phi$ would not have all necessary basis vectors to represent $V^\pi$. Under our low-dimensionality assumption $r^\pi \in \mathrm{span}(w_{i_1},\dots,w_{i_m})$, this condition implies that $\mathrm{span}(w_{i_1},\dots,w_{i_m})\subseteq \mathrm{span}(\Phi^*)$. From \cref{prop:2} we know that any critical point $\Phi^*$ of $L_\text{rec}$ satisfies $\mathrm{span}\,({\Phi^*})=\mathrm{span}\left(\{u_1,\dots,u_k\}\right)$, where $u_1,\dots,u_k$ are the top $k$ left singular vectors of $P^\pi$. Under the assumption that $\mathrm{span}(w_{i_1},\dots,w_{i_m}) \not\subseteq \mathrm{span}(u_1,\dots,u_k)$ these conditions cannot happen simultaneously, and hence no critical point of the joint loss achieves perfect value function reconstruction.
\end{proof}

\topkDistracted*
\begin{proof}
We first note that $\mathrm{span}(V_k) = \mathrm{span}(\{v_i\otimes u_1|\forall i \leq k\})$ follows directly from \autoref{lem:spectrum_kronecker} and \autoref{lem:orth_kronecker}. The eigenvector $u_1 = \vone$ as $M$ is a stochastic matrix.

As $V_k$ is an orthogonal basis, write the projection operation as 
\begin{align*}
V_k V_k^\top \left(r_\gM \otimes \vone\right) &= \frac{1}{m}\left(\vone \otimes \mathrm{orth}(V)\right)\left(\vone \otimes \mathrm{orth}(V)^\top \right)\left(r_\gM \otimes \vone\right) \\
&= \frac{1}{m}\left((\vone \otimes \vone) \otimes (\mathrm{orth}(V)\,\mathrm{orth}(V)^\top)\right) (r_\gM \otimes \vone)\\
&= \frac{1}{m}(\vone \otimes \vone) r_\gM \otimes (\mathrm{orth}(V)\,\mathrm{orth}(V)^\top) \vone \\
&= \sum_{i=1}^m \frac{r_i}{m}\vone.
\end{align*}

\end{proof}
