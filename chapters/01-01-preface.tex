\chapter*{Preface}

As with all works of science, this work stands on the shoulders of giants and has been supported generously by my collaborators and advisors.
Throughout the thesis, I make references to sources for know results following the standards of scientific research.

In this thesis, I summarize and present the results of my research conducted at the University of Toronto.
As Machine Learning has become a collaborative discipline, much of my own work has been developed together with my advisors, colleagues at the University and abroad, and the amazing students who I had the great fortune to mentor.
This thesis is built on work developed in collaboration with Amir-massoud Farahmand, Igor Gilitschenski, Animesh Garg, Eric Eaton, Marcel Hussing, Romina Abachi, Arash Ahmadian, Victor Liao, and Anastasiia Pedan.
I acknowledge and thank them for their great work.

This section clarifies the individual contributions in each chapter. With all papers, all authors were involved in editing the writing.
The chapters in this thesis are lightly edited from the original papers to provide clearer insights in the context of the overarching narrative.

\begin{itemize}
    \item \autoref{chap:overestimation}: This chapter is based on \longfullcite{hussing2024dissecting}. I contributed as joint first author and the core ideas of the work were developed in collaboration. Marcel Hussing provided the initial experiments on priming, the effects of optimizers, regularizers and conservative learning. The experiments on feature normalization were designed jointly. Writing was shared primarily between Marcel Hussign an
    \item \autoref{chap:vagram}: This chapter is based on \longfullcite{voelcker2022value}. The writing, insights, core methodology, and experiments are my contribution, Victor Liao contributed to the experimental infrastructure and code.
    \item \autoref{chap:understanding}: This chapter is based on \longfullcite{voelcker2024when}. The writing, insights, proofs, and experiments are my contribution. The proofs in \autoref{} are based on joint work with Tyler Kastner. \autoref{} was suggested by Amin Rakhsha and fully written out by me.
    \item \autoref{chap:cvaml}: This chapter is based on \longfullcite{voelcker2025calibrated}, and an older version of the paper publicized as \longfullcite{voelcker2023lambda}. The insights and most proofs are my contribution. The experimental architecture was first developed in colaboration with Romina Abachi, and Arash Ahmadian, and greatly expanded by Anastasiia Pedan. The final experiments were designed by me. \autoref{} was conjectured by me, the proof was thoroughly sketched out by Amir-massoud Farahmand, and finally written out and completed by me. \autoref{} was proposed by Amin Rakhsha and fully written out by me.
    \item \autoref{chap:mad}: This chapter is based on \longfullcite{voelcker2025mad}. I contributed as joint first author and the core ideas of the work were developed in collaboration. The initial insights of the paper were developed based on previous joint work \cite{hussing2024dissecting} together with Marcel Hussing as well as my own work in \textcite{voelcker2023lambda}. The initial experiments on the impact of model data on on- and off-policy validation loss were contributed by me. All further experiments, the core architecture and experimental setup were developed jointly.
\end{itemize}

In addition to the papers listed above, I contributed to the following reinforcement learning works during my time at the University of Toronto. These papers are not primary chapters in this thesis, and where their ideas are used, they are cited accordingly
\begin{itemize}
	\item \longfullcite{abachi2022viper}: The paper presents a latent model-based architecture that uses an IterVAML \parencite{itervaml} style loss over past value function estimates. The architecture was first implemented in \textcite{voelcker2023lambda,voelcker2025mad,voelcker2025calibrated} and the past value functions were discarded in favor of other, more computationally favorable auxiliary losses investigated in \textcite{voelcker2024when}.
	I contributed to the idea and writing of the paper.
	\item \longfullcite{guan2024temporaldifference}: The paper presents a novel architecture for training reinforcement learning agents without backpropagating TD learning signals through a neural architecture.
	As backpropagation is not a biologically plausible learning architecture, this work provides an important proof-of-feasibiity for non-backpropagation based reinforcement learning.
    I served as a reinforcement learning expert on the paper and helped developed the experiments.
	\item \longfullcite{voelcker2024can}: This paper presents insights into the impact of environment selection on the empirical evaluation of reinforcement learning algorithms.
	The core ideas of the paper were developed jointly with Marcel Hussing and experimental evaluation was done jointly.
    The insights informed environment selection for \textcite{voelcker2025mad}.
\end{itemize}
