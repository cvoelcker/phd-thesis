\chapter*{Preface}
\addcontentsline{toc}{chapter}{Preface}

In this thesis, I summarize and present the results of my research conducted at the University of Toronto.
As with all works of science, this work stands on the shoulders of giants and has been supported generously by my collaborators and advisors.
As Machine Learning has become a collaborative discipline, much of my own work has been developed together with my advisors, colleagues at the University and abroad, and the amazing students whom I had the great fortune to mentor.
This thesis is built on papers developed in collaboration with Amir-massoud Farahmand, Igor Gilitschenski, Animesh Garg, Eric Eaton, Marcel Hussing, Romina Abachi, Arash Ahmadian, Victor Liao, and Anastasiia Pedan.
I acknowledge their great contributions to the work that led to this thesis.

This section clarifies the individual contributions in each chapter. With all papers, all authors were involved in editing the writing.
The chapters in this thesis are lightly edited from the original papers to connect the individual contributions more clearly to the overarching narrative.

\begin{itemize}
    \item \autoref{chap:overestimation}: This chapter is based on \longfullcite{hussing2024dissecting}. I contributed as joint first author and the core ideas of the work were developed in collaboration. Marcel Hussing provided the initial experiments on priming, the effects of optimizers, regularizers and conservative learning. The experiments on feature normalization were designed jointly. Writing was shared equally between Marcel Hussing and I.
    \item \autoref{chap:understanding}: This chapter is based on \longfullcite{voelcker2024when}. The writing, insights, proofs, and experiments are my contribution. The proof of \autoref{prop:understanding:BYOLCombined} is joint work with Tyler Kastner. \autoref{lem:understanding:stability} was suggested by Amin Rakhsha and fully written out by me.
    \item \autoref{chap:vagram}: This chapter is based on \longfullcite{voelcker2022value}. The writing, insights, core methodology, and experiments are my contribution, Victor Liao contributed to the experimental infrastructure and code.
    \item \autoref{chap:cvaml}: This chapter is based on \longfullcite{voelcker2025calibrated}, and an older version of the paper publicized as \longfullcite{voelcker2023lambda}. The insights and most proofs are my contribution. The experimental architecture was first developed in collaboration with Romina Abachi, and Arash Ahmadian, and greatly expanded by Anastasiia Pedan. The final experiments were designed by me and implemented together with Anastasiia Pedan. \autoref{lem:cvaml:deterministic_representation_lemma} was conjectured by me, the proof was thoroughly sketched out by Amir-massoud Farahmand, and finally written out and completed by me. \autoref{lemma:cvaml:muzero} was suggested by Amin Rakhsha and fully written out by me.
    \item \autoref{chap:mad}: This chapter is based on \longfullcite{voelcker2025mad}. I contributed as joint first author and the core ideas of the work were developed in collaboration. The initial insights of the paper were developed based on previous joint work \parencite{hussing2024dissecting} together with Marcel Hussing as well as my own work in \textcite{voelcker2023lambda}. The initial experiments on the impact of model data on on- and off-policy validation loss were contributed by me. All further experiments, the core architecture and experimental setup were developed jointly. Writing was shared equally between Marcel Hussing and I.
\end{itemize}

In addition to the papers listed above, I contributed to the following technical works on reinforcement learning during my time at the University of Toronto. These papers are not primary chapters in this thesis, and where their ideas are used, they are cited accordingly
\begin{itemize}
	\item \longfullcite{abachi2022viper}: The paper proposes a latent model-based architecture that uses an IterVAML \parencite{itervaml} style loss over past value function estimates. Similar architectures were used in \textcite{voelcker2023lambda,voelcker2025mad,voelcker2025calibrated} and the past value functions were discarded in favor of other, more computationally favorable auxiliary losses investigated in \textcite{voelcker2024when}.
	I contributed to the idea and writing of the paper.
	\item \longfullcite{guan2024temporaldifference}: The paper presents a novel architecture for training reinforcement learning agents without backpropagating TD learning signals through a neural architecture.
	As backpropagation is not a biologically plausible learning architecture, this work provides an important proof-of-feasibility for non-backpropagation based reinforcement learning.
    I served as a reinforcement learning expert on the paper and helped developed the experiments.
	\item \longfullcite{voelcker2024can}: This paper presents insights into the impact of environment selection on the empirical evaluation of reinforcement learning algorithms.
	The core ideas of the paper were developed jointly with Marcel Hussing and experimental evaluation was done jointly.
    The insights informed environment selection for \textcite{voelcker2025mad}.
\end{itemize}

All illustrations in this thesis are my original work, graphs and tables are taken from the acknowledged papers.