Reinforcement learning (RL) is one of the most promising pathway towards building decision-making systems that can learn from their own successes and mistakes.
However, despite their potential, RL agents often struggle to learn complex tasks, proving too inefficient and unstable in practice.
To enable RL-based agents to live up to their potential, we need to address these limitation.
Addressing this challenge is the focus of this thesis.

To accomplish this, we unveil the mechanisms that lead to unstable and inefficient value function learning.
Learned value functions tend to overestimate real returns during training, and that this overestimation is linked to unstable learning in the feature representation layers of neural networks.
To counteract this, we show the need for proper normalization of learned value approximations.
Building on this insight, we then investigate model-based auxiliary tasks to stabilize feature learning further.
We find that model-based self-prediction, in combined with a value learning loss, leads to stable features.

Moving beyond feature learning, we investigate the paradigm of decision-aware model learning.
We find that, similar to the issues encountered in representation learning, tying model updates to the value function can lead to unstable and even diverging model learning.
This problem can be mitigated in observation-space models by using the value function gradient to measure its sensitivity with regard to model errors.
We then move on to combine our insights into representation learning and model learning.
We discuss the family of value-aware model learning algorithms and show how to extend its losses to account for learning with stochastic models.
Finally, we show that combining all previous insights in a unified architecture can lead to stable and efficient value function learning.