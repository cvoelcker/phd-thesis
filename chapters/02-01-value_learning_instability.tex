\chapter{Instability of Value Function Learning}
\label{chap:overestimation}

\begin{quote}
    This chapter is based on \longfullcite{hussing2024dissecting}.
\end{quote}

To improve sample efficiency, contemporary work in off-policy deep reinforcement learning (RL) has begun increasing the number of gradient updates per collected environment step~\parencite{janner2019mbpo,fedus2020revisiting,chen2021randomized, hiraoka2022dropout, nikishin2022primacy, doro2023barrier, schwarzer2023bigger, kim2023resetensemble}.  
However, as this update-to-data (UTD) ratio increases, value function learning seems to become unstable.
This is surprising, as neural networks have shown themselves to be generally robust to the number of update steps in other areas such as supervised classification.
In some cases, training for a very large number of steps actually increases the performance of neural networks, due to phenomena such as grokking \parencite{power2022grokking} or double descent \parencite{belkin2019reconciling}.
As such, this phenomenon should raise our suspicion that training \ac{drl} agents causes issues with neural networks that are not observed in other domains.

Notably, a recent study conjectured the emergence of a \emph{primacy bias} in deep actor critic algorithms, defined as ``a tendency to overfit initial experiences that damages the rest of the learning process''~\parencite{nikishin2022primacy}. 
This is a fairly broad explanation of the phenomenon, leaving room for investigation into \emph{how} fitting early experiences causes suboptimal learning behavior.

First approaches to tackle the learning failure challenges have been suggested, such as completely resetting networks periodically during the training process and then retraining them using the contents of the replay buffer~\parencite{nikishin2022primacy, doro2023barrier}. 
Resetting network parameters is a useful technique in that, in some sense, it can circumvent any ongoing optimization failures. 
Yet it seems likely that a more nuanced treatment of the various optimization challenges in deep RL might lead to more efficient training down the line, and provide more useful insights into how learning in \ac{drl} evolves more generally. 
Especially for achieving efficient learning, throwing away all learned parameters and starting from scratch periodically is counter-productive, for instance in scenarios where, keeping all previous experience is infeasible. %
As such, we set out to study the components of early training that impair learning more closely and examine whether high-UTD learning without resetting is possible.


To motivate our study, we repeat the priming experiment of \textcite{nikishin2022primacy}, in which a network is updated for a large number of gradient steps on limited data. We show that during priming stages of training, value estimates diverge so far -- and become so extreme -- that it takes very long to unlearn them using new 
data. However, contrary to prior work, we find that it is not \emph{impossible} to learn even after priming, it merely takes a long time and many samples. 
This sparks hope for our endeavor of smooth learning in high-UTD regimes.
We show that preventing value function divergence allows learning to proceed. 
This suggests that the failure to learn does not stem from overfitting early data, which would result in correct value function on seen data, but rather from improperly fitting Q-values. 
We demonstrate that this divergence is most likely caused by prediction of out-of-distribution (OOD) actions that trigger large gradient updates, which destabilize feature learning.
Empirically, we find that instead of settling into fixed representations, the norm of the learned representations in the intermediate layers of the critic neural network grows.

The identified behavior, although triggered by OOD action prediction, seems to be more than the well-known overestimation due to statistical bias \parencite{thrun1993issues}. 
Instead, we hypothesize that the problem is an optimization failure and focus on mitigating the unbounded growth of the feature norm with architectural changes.
The main evidence for this hypothesis is that standard RL approaches to mitigating bias, such as minimization over two independent critic estimates~\parencite{fujimoto2018addressing}, are insufficient. 
In addition, using pessimistic updates~\parencite{fujimoto2019bcq, fujimoto2021td3bc} or regularization~\parencite{krogh1991simple, srivastava14dropout} to treat the value divergence can lead to suboptimal learning behavior, which is why architectural improvements are preferable in many cases.


We show that a simple feature normalization method, unit-ball projection \parencite{bjorck2022is}, is sufficient to prevent catastrophic overestimation due to divergence.
Unit-ball projection decouples learning the scale of the values from the first layers of the network and moves it to the last linear layer.
Empirically, this approach mitigates diverging Q-values in the priming experiment. 
Even after a large amount of priming steps, the agent immediately starts to learn. %
In a set of experiments on the \textsf{dm\_control} MuJoCo benchmarks~\parencite{tunyasuvunakool2020dmcontrol}, we show that accounting for value divergence can achieve significant across-task performance improvements when using high update ratios. 
Moreover, we achieve non-trivial performance on the challenging dog tasks that are often only tackled using model-based approaches. We demonstrate comparable performance with the recently developed TD-MPC2~\parencite{hansen2024tdmpc}, without using models or advanced policy search methods.


\section{Preliminaries} \label{sec:overestimation:preliminaries}

To fully elucidate the role that increasing the update ratio plays in destabilizing value function training, ostensibly necessitating resets, we present a series of small-scale experiments.

\subsection{Investigating large update-to-data ratios during priming} \label{sec:overestimation:investigating}


As mentioned, the definition of the primacy bias is broad.
To obtain a more nuanced understanding, we set out to re-investigate the early stages of high-UTD training. To do so, we repeat the priming experiment conducted by~\textcite{nikishin2022primacy}.

We first collect 10,000 environment samples with a random policy. 
Then, using the SAC algorithm, we update the network during the priming phase.
This means that we train the agent for a large number of gradient update steps without collecting additional data.
After the priming phase, training continues as usual, interleaving gradient updates and environment interaction steps.

Prior results reported by~\textcite{nikishin2022primacy} suggest that once the priming step has happened, agents lose their ability to learn.
They suggest to reset the trained networks to recover the ability to improve.
Here, we repeat this experiment using the simple Finger-spin task \parencite{tunyasuvunakool2020dmcontrol} to study the root causes for this failure and to examine if there are ways to recover without resets.
In this section, we report means over five random seeds with bootstrapped confidence intervals at 95\% in shaded regions. 
Hyperparameters are kept consistent with previous work for ease of comparison.

\begin{figure}[t]
\centering
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[height=0.4cm]{figures/dissecting/priming/priming_base_return_legend.pdf}
    \end{subfigure}\\%
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/dissecting/priming/priming_base_return.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
    \centering
        \includegraphics[width=\textwidth]{figures/dissecting/priming/priming_base_Q.pdf}
    \end{subfigure}
    % \hfill
    % \begin{subfigure}[b]{0.24\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{figures/dissecting/priming/priming_base_exp_avg.pdf}
    % \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/dissecting/priming/priming_base_exp_avg_sq.pdf}
    \end{subfigure}%
    \caption{Return, in-distribution Q-values and Adam optimizer moments during priming for different lengths. Dotted lines correspond to end of priming. More priming steps lead to lower return and larger Q-values.}
    \label{fig:overestimation:priming_base}
\end{figure}

% \begin{figure}[b]
% % \captionsetup[subfigure]{font=footnotesize, aboveskip=2pt}
% \centering
%     \begin{subfigure}[b]{0.8\textwidth}
%         \centering
%         \includegraphics[height=0.4cm]{figures/dissecting/priming/elu_dim_return_legend.pdf}
%     \end{subfigure}\\%
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=4.7cm, trim=1cm 1cm 1cm 1cm ,clip]{figures/dissecting/priming/elu_dim_return.pdf}
%         \label{subfig:elu_dim_ret}
%     \end{subfigure}%
%     \begin{subfigure}[b]{0.33\textwidth}
%     \centering
%         \includegraphics[width=4.7cm, trim=1cm 1cm 1cm 1cm ,clip]{figures/dissecting/priming/elu_dim_Q.pdf}
%         \label{subfig:priming_dim_Q}
%     \end{subfigure}%
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=4.7cm, trim=1cm 1cm 1cm 1cm ,clip]{figures/dissecting/priming/elu_dim_ed.pdf}
%         \label{subfig:overestimation:elu_priming_dim_ed}
%     \end{subfigure}%
%     \vspace{-5pt}
%     \caption{Returns, mean Q-values and effective dimension for $3$ seeds of standard priming for 75,000 steps. When divergence happens, effective dimension is lost. If the effective dimension drops too far, the agent has difficulties to recover.}
%     % \caption{Mean return, Q-values and Adam moments for priming of different lengths. Dotted lines indicate end of priming. More priming leads to lower return and Q-value and optimizer divergence.}
%     \label{fig:overestimation:elu_priming_dim}
% \end{figure}

\label{sec:overestimation}
We first ask whether there is a barrier as to how many update steps can be executed in the priming phase before the agent becomes unable to improve in the regular training phase.
We test this by collecting an initial batch of 1,000 samples and varying the number of updates during priming from 25,000 to 50,000 and 100,000. The results are presented in Figure~\ref{fig:overestimation:priming_base}. 

We make two key observations. First, lower amounts of priming are correlated with higher early performance. More precisely, it seems that many runs simply take longer before they start achieving non-zero returns as the number of priming steps increases. Second, during priming the scale of the average Q-value estimates on observed state-action pairs increases drastically. We find that the Q-values begin close to the achieved returns, but as priming goes on they eventually start to diverge drastically. Once the agent estimates very large Q-values, the final performance in terms of average returns deteriorates. 
% We also observe that the second moment of the Adam optimizer~\parencite{kingma2015adam} is correlated with the divergence effect. Optimizer divergence has been observed before as a cause of plasticity loss under non-stationary data \parencite{lyle2023understanding}, but in our experiments the data is stationary during priming. We conjecture that the momentum terms lead to much quicker propagation of poor Q-values and ultimately to prediction of incorrect Q-values, even on in-distribution data.

% \subsection{Effective dimension}
% Furthermore, the plasticity loss hypothesis has been linked to the loss of effective dimension or rank in the literature \parencite{kumar2021implicit}.
% We therefore take a look at the evolution of the effective dimension over the course of priming.
% 
% Let $\Phi \in \mathbb{R}^{|\states| |\mathcal{A}| \times d}$ be a feature matrix (in our case produced by $\phi$). \textcite{yang2020harnessing} defines the effective dimension of a feature matrix as 
% \begin{equation*}
%     \text{srank}_{\delta} = \min 
%     \left\{ k : \cfrac{\sum_{i=1}^k \sigma_i(\Phi)}{\sum_{i=1}^d \sigma_i(\Phi)} \geq 1 - \delta
%     \right\} \enspace ,
% \end{equation*}
% where $\delta$ is a threshold parameters and $\{\sigma_i(\Phi)\}$ are the singular values of $\Phi$ in decreasing order.
% 
% We find that divergence of Q-values is correlated with this effective rank $\text{srank}_{\delta}$. We plot three different random seeds that have been subjected to 75,000 steps of priming in Figure~\ref{fig:overestimation:elu_priming_dim}; the effective rank is approximated over a sample 10 times the size of the embedding dimension. We observe, that divergence correlates with a decrease in effective dimension and that when divergence is exceptionally strong, the effective dimension drops so low that the agent has trouble to continue learning. This might explain the failure to learn observed by~\textcite{nikishin2022primacy}. However, as long as the effective dimension does not drop too far, the agent can recover and regain capacity by observing new data. 
% Previous work on effective rank loss has often assumed that it is mostly irreversible, yet we find that this is not always the case.
% We suspect that in complete failure cases, the policy has collapsed and rarely any new data is seen.
% 
% 
% 
% 
% After priming, there exist two cases: 1) either the Q-values need to be unlearned before the agent can make progress or 2) there is a large drop from very high to very low Q-values that is strongly correlated with loss in effective dimension of the embedding, as defined by~\textcite{yang2020harnessing}. In the second case, we find that contrary to previous claims in the literature, rank can sometimes be recovered upon seeing new, counter-factual data and the network continues to learn. Yet, in other cases the agent gets stuck at low effective dimension; a possible explanation for the failure to learn observed in the priming experiments of~\textcite{nikishin2022primacy}. This is orthogonal to a previously studied phenomenon where target network-based updates lead to perpetually reduced effective rank~\parencite{kumar2021implicit}.


\subsection{On the potential causes of divergence} \label{sec:causes}

\begin{figure}[b]
\begin{minipage}[b]{.31\textwidth}
\centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[height=0.7cm]{figures/dissecting/priming/priming_causes_return_legend.pdf}
    \end{subfigure}\\%
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/dissecting/priming/priming_causes_Q.pdf}
    \end{subfigure}%
    \caption{Priming with SAC and action regularization during priming. The latter lowers divergence. }
    \label{fig:overestimation:priming_bc}
\end{minipage}
\hfill
\begin{minipage}[b]{.62\textwidth}
    \centering
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[height=0.7cm]{figures/dissecting/priming/priming_ablations_return_legend.pdf}
    \end{subfigure}\\%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/dissecting/priming/priming_ablations_return.pdf}
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
    \centering
        \includegraphics[width=\textwidth]{figures/dissecting/priming/priming_ablations_Q.pdf}
    \end{subfigure}%
    \caption{Return and
    Q-values of priming runs with weight decay and dropout. Results indicate that both regularizers mitigate priming to some extent but not sufficiently. }
    \label{fig:overestimation:priming_abl}
\end{minipage}
\end{figure}


We conjecture that Q-value divergence starts with overestimated values of OOD actions. This overestimation could cause the optimizer to continually increase Q-values via its momentum leading to divergence. 
To test this hypothesis, we add a conservative behavioral cloning~\parencite{pomerleau1988alvinn, atkeson1997robot} loss term to our actor that forces the policy to be close to replay buffer actions. Prior work employed this technique in offline RL to mitigate value overestimation~\parencite{fujimoto2021td3bc}. More formally, our actor update is extended by the loss $
        \mathcal{L}_{c, \phi} = \min_{\phi} \mathbb{E}_{a \sim D, \hat{a} \sim \pi_{\phi}(\state)}[\|a - \hat{a}\|_2]$. 
The results in Figure~\ref{fig:overestimation:priming_bc} indicate that the basis of the conjecture is corroborated as divergence is much smaller---but not mitigated completely---when values are learned on actions similar to seen ones.
We will revisit this hypothesis in \autoref{chap:mad}.
However, in practice we do not know when divergence sets in, which limits the applicability of this technique in realistic scenarios. Using it throughout all of training, rather than just during priming, impairs the learner's ability to explore.




\subsection{Applying common regularization techniques} \label{sec:regularization}

Regularization is a common way to mitigate gradient explosion and is  often used to address overestimation~\parencite{farebrother2018generalization, chen2021randomized, liu2021regularization, hiraoka2022dropout, li2023efficient}. We investigate the priming experiments under techniques such as using weight decay~\parencite{krogh1991simple} or adding dropout~\parencite{srivastava14dropout} to our networks in Figure~\ref{fig:overestimation:priming_abl}.

Both weight decay as well as dropout can somewhat reduce the divergence during priming, however not to a sufficient degree.
While we fail to attain strong performance with weight decay, dropout is able to recover a good amount of final return.
However, both methods require tuning of a hyperparameter that trades off the regularization term with the main loss.
This hyperparameter is environment-dependent and tuning it becomes infeasible for large UTD-ratios due to computational resource limitations. 
Still, the results imply that it is in fact possible to overcome the divergence in priming and continue to learn good policies.





\subsection{Divergence in practice} 
\label{sec:real}


\begin{figure}[b]
    \centering
    \includegraphics[width=0.33\textwidth]{figures/dissecting/priming/humanoid_Q.pdf}
    \includegraphics[width=0.33\textwidth]{figures/dissecting/priming/humanoid_loss.pdf}
    \caption{In-distribution Q-values and critic loss of five SAC seeds on the humanoid-run task using $\mathrm{UTD}=32$. Values diverge at arbitrary time-points, not only during the beginning. Loss mirrors Q-value divergence.}
    \label{fig:overestimation:humanoid_failure}
\end{figure}

One question that remains is whether we can find these divergence effects outside of the priming setup. We find that, while priming is an artificially constructed worst case, similar phenomena happen in regular training on standard benchmarks when increasing update ratios (see Figure~\ref{fig:overestimation:humanoid_failure}). Further, the divergence is not limited to early stages of training as it happens at arbitrary points in time.

We therefore conjecture that divergence is not solely a consequence of the amount of experience gathered but rather happens due to state-action space coverage. Note that the reported Q-values have been measured on the training data, not on a separate validation set. 
The respective critic losses become very large. All this points toward a failure to fit Q-values. This behavior also does not align with our common understanding of overfitting~\parencite{bishop2006pattern},  challenging the hypothesis that high-UTD learning fails merely due to large validation error~\parencite{li2023efficient}.

\section{Towards high-UTD optimization without resetting} \label{sec:method}

Regularization techniques such as those in Section~\ref{sec:regularization} can fail to alleviate divergence as they tend to operate across the whole network and lower the weights everywhere even if higher values are actually indicated by the data. They also require costly hyperparameter tuning. Thus, we turn towards network architecture changes to the commonly used fully-connected feed-forward networks that have proven useful in overcoming issues such as exploding gradients in other domains~\parencite{ba2016layer, xu2019understanding}. 

\subsection{Limiting gradient explosion via unit-ball normalization} \label{sec:unitball}


\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \includegraphics[width=0.5\textwidth]{illustrations/thesis_dissecting_norm_vis.pdf}
    \caption{A graphical representation of output feature normalization (OFN). Each feature vector is projected unto the unit sphere.}
\end{wrapfigure}

As discussed previously, the prediction of an unknown action might trigger the propagation of a large, harmful gradient. Further, the Q-values of our network ought to grow over time as they more closely approximate those of a good policy. If we predict incorrectly on one of these Q-values, a potentially very large loss is propagated. Gradients are magnified by multiplicative backpropagation via ReLU activations~\parencite{glorot2011deep} as well as momentum from Adam~\parencite{kingma2015adam}. Note that all resulting issues arise in the early network layers. 
We hypothesize that we can address many of these problems by separating the scaling of the Q-values to the appropriate size from the earlier non-linear layers of the network and moving the Q-value scaling to the final linear layer.

One contender to achieve the value decoupling described in the previous paragraph is layer normalization~\parencite{ba2016layer}, but one would have to disable scaling factors used in common implementations. Still, standard layer normalization would not guarantee small features everywhere. Instead, we use a stronger constraint and project the output features of the critic encoder onto the unit-ball using the function
$f(\mathbf{z}) = \frac{z}{\|z\|_2}$~\parencite{zhang2019root}, 
where $\|\cdot\|_2$ denotes the $L_2$ vector norm and $z$ is the output of our encoder $\phi(\state, a)$. This ensures that all values are strictly between $0$ and $1$ and the gradients will be tangent to the unit sphere. Note that this function's gradient is not necessarily bounded to ensure small gradients for prior layers (see Appendix~\ref{app:unitnorm}), but we observe that empirically that gradient explosion does not occur. 
The unit-ball has previously been used to mitigate large action prediction in the actor~\parencite{wang2020striving} or to stabilize RL training in general~\parencite{bjorck2022is}.
Notably, while \textcite{bjorck2022is} discusses the reasons for using unit-ball regularization for policy learning, they present results for using unit-ball normalization for value learning without further discussion.
Therefore our work here can be seen as a thorough analysis of a previously unsupported ad-hoc trick. 
For brevity, we will refer to this method as {\em output feature normalization} (OFN). We solely apply OFN to the critic, unlike~\textcite{wang2020striving}, since our goal is to mitigate value divergence. OFN is very simple and requires only a one-line change in implementation.

\subsection{Evaluating feature output normalization during priming} \label{sec:evalmethod}

\begin{figure}[t]
\begin{minipage}[t]{.60\textwidth}
\centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[height=0.4cm]{figures/dissecting/priming/priming_norm_return_legend.pdf}
    \end{subfigure}\\%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=4.8cm]{figures/dissecting/priming/priming_norm_return.pdf}
        \label{subfig:overestimation:priming_norm_ret}
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=4.8cm]{figures/dissecting/priming/priming_norm_Q.pdf}
        \label{subfig:overestimation:priming_norm_Q}
    \end{subfigure}%
    \caption{(Left) Return and (Right) Q-values comparing SGD result and OFN when priming for 100K steps. OFN obtains returns close to that of the well-trained SGD agent and learns an appropriate Q-value scale correctly.}
    \label{fig:overestimation:priming_norm}
\end{minipage}
\hfill
\begin{minipage}[t]{.36\textwidth}
\centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[height=0.8cm]{figures/dissecting/priming/weight_magnitudes_legend.pdf}
    \end{subfigure}
    \\%
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=4.6cm]{figures/dissecting/priming/weight_magnitudes.pdf}
    \end{subfigure}%
    \caption{$L_2$ norm of network weights per layer after priming for default and OFN  architectures. OFN leads to smaller weights, especially for earlier layers.}
    \label{fig:overestimation:weight_norm}
\end{minipage}
\end{figure}

To test the efficacy of the OFN-based approach, we repeat the priming experiment in Figure~\ref{fig:overestimation:priming_norm}.
We find that OFN achieves high reward and, more importantly, Q-value divergence during priming is fully mitigated. 
We therefore expect the average Q-values to be roughly at $10\%$ of the undiscounted return which seems correct for the OFN network. 
In addition, as shown in Figure~\ref{fig:overestimation:weight_norm}, the weight norm of all layers of the networks are smaller, with the largest weights in the final layer.

\section{Experimental evaluation}

We evaluate our findings on the commonly used \textsf{dm\_control} suite~\parencite{tunyasuvunakool2020dmcontrol}. All results are averaged over ten random seeds.\footnote{For comparison with TD-MPC2~\parencite{hansen2024tdmpc} we use data provided by their implementation, which only contains three seeds. As the goal is not to rank algorithmic performance but to give intuition about the relative strengths of adapting the network architecture, we believe that this is sufficient in this case.} We report evaluation returns similar to~\textcite{nikishin2022primacy}, which we record every 10,000 environment steps. We compare a standard two-layer fully-connected feed-forward network with ReLU ~\parencite{nair2010rectified} activations, both with and without resetting, to the same network with OFN. %
The architecture is standard in many reference implementations. Architecture and the resetting protocol are taken from \textcite{doro2023barrier} and hyperparameters are kept without new tuning to ensure comparability of the results. More details can be found in Appendix~\ref{app:impl}.

To understand the efficacy of output normalization on real environments under high UTD ratios, we set out to answer multiple questions that will illuminate RL optimization failures:\\
{\bf Q1:}~Can we maintain learning without resetting neural networks?\\
{\bf Q2:}~Are there other failure modes beside Q-value divergence under high UTD ratios?\\
{\bf Q3:}~When resets alone fall short, can architectural changes enable better high-UTD training?

\subsection{Feature normalization stabilizes high-UTD training} 

\begin{figure}[t]
\begin{minipage}[b]{0.67\textwidth}
\centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=10.2cm]{figures/dissecting/main_exp/all_aggregate_scores.pdf}
    \end{subfigure}%
    \caption{Mean, interquartile mean (IQM), and median with $95\%$ bootstrapped confidence intervals of standard SAC and OFN on the DMC15-500k Suite. OFN can maintain high performance even under large UTD. OFN with $\mathrm{UTD} = 8$ achieves comparable performance to standard resetting with $\mathrm{UTD} = 32$ across metrics.}
    \label{fig:overestimation:aggregate}
\end{minipage}
\hfill
\begin{minipage}[b]{.3\textwidth}
\centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/dissecting/main_exp/hopper_hop_wl.pdf}
    \end{subfigure}%
    \caption{Mean return of single-critic OFN, standard OFN and resetting; $\mathrm{UTD}=32$ on hopper-hop. Shaded regions are bootstrapped confidence intervals.}
    \label{fig:overestimation:hopper_hop}
\end{minipage}
\end{figure}



To answer \textbf{Q1}, we compare OFN and SAC with resets on the DMC15-500k benchmark with large update ratios of $8$ and $32$ as proposed by \textcite{nikishin2022primacy} and  \textcite{schwarzer2023bigger}.  We report mean, interquartile mean (IQM) and median as well as $95\%$ bootstrapped confidence intervals aggregated over seeds and tasks,  following~\textcite{agarwal2021deep}. The results are shown in Figure~\ref{fig:overestimation:aggregate}.

First, we observe that in both cases, $\mathrm{UTD}=8$ and $\mathrm{UTD}=32$, OFN can significantly improve over the non-resetting baseline across all metrics. The value estimates that diverge seem to have been handled properly (see Appendix~\ref{app:exp_q}); learning is maintained. We note that our approach with $\mathrm{UTD}=8$ achieves mean and IQM performance comparable to that of standard resetting with $\mathrm{UTD}=32$. In median and quantile performance, all $\mathrm{UTD}=32$ overlap, highlighting that outliers contribute to the performance measurement. Note that the overall performance drops slightly for the OFN-based approach when going from $\mathrm{UTD}=8$ to $\mathrm{UTD}=32$. We conjecture that there are other learning problems such as exploration that have not been treated by alleviating value divergence. However, these do not lead to complete failure to learn but rather slightly slower convergence. %

\subsection{Other failure modes: Exploration limitations} \label{sec:otherfailure}


To validate the hypothesis of other failures and answer \textbf{Q2}, we run two additional experiments. First, our current focus is on failures of the critic; our proposed mitigation does not address any further failures that might stem from the actor. To understand the actor's contribution to the performance increases from resetting, we test the OFN-based architecture again and, simply reset the actor.
For comparison, we also run a second experiment in which we reset all learned parameters, including the critic.
This settings are referred to as ``Actor Reset'' and ``Full Reset'' respectively in \autoref{fig:overestimation:aggregate}.

The results in Figure~\ref{fig:overestimation:aggregate} indicate that actor resetting can account for a meaningful portion of OFN's performance decline when going from $\mathrm{UTD}=8$ to $\mathrm{UTD}=32$. The actor-reset results are within variance of the full-resetting standard baseline. Further, we observe that there is still some additional benefit to resetting the critic as well. 
This does not invalidate the premise of our hypothesis, value divergence might not be the \emph{only} cause of problems in the high UTD case. We have provided evidence that it is a \emph{major} contributor.
Resetting both networks of OFN with $\mathrm{UTD}=32$  outperforms all other baselines on mean and IQM comparisons. %

To explain the remaining efficacy of critic resets, we examine the hopper-hop environment where standard SAC with resets outperforms OFN. 
In RL with function approximation, one might not only encounter over- but also under-estimation~\parencite{wu2020reducing, lan2020maxmin, saglam2021estimation}.
We believe that hopper  is sensitive to pessimism, and periodically resetting the networks might partially and temporarily counteract the inherent pessimism of the dual critic setup.

To corroborate this conjecture, we repeated some experiments with a single critic. As OFN handles divergence it might not require a minimization over two critics~\parencite{fujimoto2018addressing}. We compare OFN using a single critic and $32$ updates per environment step to standard SAC and OFN in Figure~\ref{fig:overestimation:hopper_hop}. With a single critic, OFN does not get stuck in a local minimum and outperforms full resetting. Note that this is only true in few environments, leading us to believe that the effects of high-update training are MDP-dependent.
In some environments we observe unstable learning with a single critic, which highlights that the bias countered by the double critic optimization and the overestimation from optimization we study are likely orthogonal phenomena that both need to be addressed.
% Most likely, there is a difficult trade-off between optimization stability and encouraging sufficient exploration, which is an exciting avenue for future research.

\subsection{Limit-testing feature normalization}

\begin{figure}[t]
\centering
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[height=0.4cm]{figures/dissecting/dog_exp/dog_legend.pdf}
    \end{subfigure}\\%
    \begin{subfigure}[t]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/dissecting/dog_exp/dog-stand.pdf}
    \end{subfigure}%
    \begin{subfigure}[t]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/dissecting/dog_exp/dog-walk.pdf}
    \end{subfigure}%
    \begin{subfigure}[t]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/dissecting/dog_exp/dog-trot.pdf}
    \end{subfigure}%
    \begin{subfigure}[t]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/dissecting/dog_exp/dog-run.pdf}
    \end{subfigure}%
    \caption{Mean return on the dog DMC tasks, comparing OFN to SAC with resets and the model-based TD-MPC2. Shaded regions indicate bootstrapped confidence intervals. OFN outperforms SAC with resets, which is unable to learn and OFN with $\mathrm{UTD}=8$ and resetting is competitive with TD-MPC2.}
    \label{fig:overestimation:all_dog}
\end{figure}

To answer \textbf{Q3}, we move to a set of training environments that is considered exceptionally hard for model-free approaches, namely the dog tasks of the DMC suite. Standard SAC can generally not obtain any reasonable reward and, due to their complex dynamics, these tasks are often tackled using model-based approaches such as TD-MPC2~\parencite{hansen2024tdmpc} with complicated update procedures and carefully tuned network architectures. We evaluate SAC and OFN on the dog tasks and compare against TD-MPC2 in Figure~\ref{fig:overestimation:all_dog}.

First, observe that resetting without OFN does not result in only minor performance increases over the course of training. However, OFN with $\mathrm{UTD}=1$ can already obtain very good performance across all tasks, indicating that a major problem for SAC in these high-dimensional tasks is value divergence. When increasing the update ratio to $8$ and adding resetting, we improve the performance of the OFN agent even further and can match the reported results of the strong model-based TD-MPC2 baseline. 

We have already seen that resetting can take care of multiple optimization failures. However, these experiments also indicate that resetting is not a panacea as it is only effective when the initially learned policy can obtain some reward before being overwritten. 
This seems intuitive since resetting to a policy that cannot gather any useful data should not help. 
These results highlight that the early training dynamics of RL are highly important when it comes to training on complex environments and fitting early data correctly and quickly is crucial for success. 

This also opens up the question why resetting in the humanoid environments in the previous sections can yield success even though very little reward is observed. 
Besides greater divergence due to larger observation spaces in the dog MDPs, we suspect that this might be related to the complexity of exploration.
The ability of a random policy to obtain non-trivial reward and information about the environment has been shown to be a crucial factor in explaining the success of DRL methods in discrete environments~\parencite{laidlaw2023bridging}, and similar phenomena might be in effect here. 




\section{Related work}

Our work closely examines previous work on the primacy bias and the related resetting technique \parencite{anderson1992qlearning,nikishin2022primacy,doro2023barrier,schwarzer2023bigger}.
Going beyond, overestimation and feature learning challenges are a widely studied phenomenon in the literature.

{\bf Combatting overestimation}~~
Overestimation in off-policy value function learning is a well-established problem in the RL literature that dates back far before the prime times of deep learning~\parencite{thrun1993issues, precup2001off}. 
% The effects of function approximation error and the effect on variance and bias have been studied~\parencite{pendrith1997estimator, mannor2007bias} as well.
With the rise of deep learning, researchers have tried to address the overestimation bias via algorithmic interventions such as combining multiple Q-learning predictors to achieve underestimation~\parencite{hasselt2010double, hasselt2016deep, zongzhang2017weighted, lan2020maxmin}, using averages over previous Q-values for variance reduction~\parencite{anschel2017averaged}, or general error term correction~\parencite{donghun2013bias, fox2016taming}. In the context of actor-critic methods, the twinned critic minimization approach of \textcite{fujimoto2018addressing} has become a de-facto standard. 
Most of these approaches are either computationally expensive, such as training Q function ensembles, or break down under very high update ratios, such as in the case of the pessimistic twinned Q learning trick.
To regulate the overestimation-pessimism balance more carefully, several authors have attempted to use larger ensembles of independent Q-value estimates \parencite{lee2021sunrise,peer2021ensemble,chen2021randomized,hiraoka2022dropout}. Ensembling ideas were also combined with ideas from distributional RL~\parencite{bellemare2017distributional} to combat overestimation~\parencite{kuznetsov2020controlling}.  Instead of addressing the statistical bias in deep RL, our study focuses on the problems inherent to neural networks and gradient based optimization for value function estimation. Work from offline-to-online RL has demonstrated that standard layer normalization can bound value estimates during offline training and mitigate extrapolation while still allowing for exploration afterwards~\parencite{ball2023efficient}. Layer normalization has subsequently been used to achieve generally strong results in offline RL~\parencite{tarasov2023rebrac}. Our work is also related to a recent contribution using batch-normalization for increased computational efficiency by~\textcite{bhatt2024crossq} who focus on decreasing update ratios rather than increasing them. A concurrent work by~\textcite{nauman2024overestimation} provides a large scale study on different regularization techniques to combat overestimation. This work also demonstrates the efficacy of SAC on the dog tasks when properly regularized but it does not highlight the effects of Q-value divergence from exploding gradients as a key challenge for this set of environments.

{\bf Combating plasticity loss}~~ Another aspect of the primacy bias is the tendency of neural networks to lose their capacity for learning over time~\parencite{igl2021transient}, sometimes termed \emph{plasticity loss}~\parencite{lyle2022understanding, abbas2023loss}. Recent work mitigates plasticity loss using feature rank maximization~\parencite{kumar2021implicit}, regularization~\parencite{lyle2023understanding}, or learning a second copied network~\parencite{nikishin2024deep}. Some of the loss stems from neurons falling dormant over time~\parencite{sokar2023dormant}. A concurrent, closely related work by~\textcite{lyle2024disentangling} disentangles the causes for plasticity loss further. They use layer normalization to prevent some of these causes, which is closely related to our unit-ball normalization. Our work differs in that we focus on the setting of high update ratios and use stronger constraints to mitigate value divergence rather than plasticity loss. 


\section{Conclusion}

By dissecting the effects underlying the primacy bias, we have identified a crucial challenge: \emph{value divergence}. 
While the main focus in studying increased Q-value has been on the statistical bias inherent in off-policy sampling, we show that Q-value divergence can arise due to problems inherent to neural network optimization.
This optimization-caused divergence can be mitigated using the unit-ball normalization approach, which shines on the \textsf{dm\_control} benchmark with its simplicity and efficacy. 
With this result, we challenge the assumption that failure to learn in high-UTD settings primarily stems from \emph{overfitting} early data by showing that combating value divergence is competitive with resetting networks. 
This offers a starting point towards explaining the challenges of high-UTD training in more detail and opens the path towards even more performant and sample efficient RL in the future.

This chapter shines a light on the important role that the implicit representations learned from end-to-end critic optimization play in value function learning.
In essence, model-free value learning is a purely \emph{decision-aware} method, as it solely focuses on extracting good features for the current value prediction target.
But due to the deep entanglement between optimization, exploration, and non-stationary data, we can see that relying solely on end-to-end learning for value estimation is a potentially perilous endeavour.
While simple regularization can already alleviate some of the issues with value learning,  the next chapter takes an in-depth look at more explicit ways of constructing good representative features.
