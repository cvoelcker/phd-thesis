\chapter{Instability of value function learning}
\label{chap:overestimation}

To improve sample efficiency, contemporary work in off-policy deep reinforcement learning (RL) has begun increasing the number of gradient updates per collected environment step~\parencite{janner2019mbpo,fedus2020revisiting,chen2021randomized, hiraoka2022dropout, nikishin2022primacy, doro2023barrier, schwarzer2023bigger, kim2023resetensemble}.  
As this update-to-data (UTD) ratio increases, various novel challenges arise.
Notably, a recent study proposed the emergence of a \emph{primacy bias} in deep actor critic algorithms, defined as ``a tendency to overfit initial experiences that damages the rest of the learning process''~\parencite{nikishin2022primacy}. 
This is a fairly broad explanation of the phenomenon, leaving room for investigation into how fitting early experiences causes suboptimal learning behavior.

First approaches to tackle the learning failure challenges have been suggested, such as completely resetting networks periodically during the training process and then retraining them using the contents of the replay buffer~\parencite{nikishin2022primacy, doro2023barrier}. 
Resetting network parameters is a useful technique in that, in some sense, it can circumvent any previous optimization failures without prior specification. 
Yet it seems likely that a more nuanced treatment of the various optimization challenges in deep RL might lead to more efficient training down the line. 
Especially if the objective is efficiency, throwing away all learned parameters and starting from scratch periodically is counter-productive, for instance in scenarios where, keeping all previous experience is infeasible. %for instance when data buffer sizes become limited. 
As such, we set out to study the components of early training that impair learning more closely and examine whether high-UTD learning without resetting is possible. 


To motivate our study, we repeat the priming experiment of \textcite{nikishin2022primacy}, in which a network is updated for a large number of gradient steps on limited data. We show that during priming stages of training, value estimates diverge so far---and become so extreme---that it takes very long to unlearn them using new, counter-factual 
data. However, contrary to prior work, we find that it is not \emph{impossible} to learn even after priming, it merely takes a long time and many samples. 
This sparks hope for our endeavor of smooth learning in high-UTD regimes.
We show that compensating for the value function divergence allows learning to proceed. This suggests that the failure to learn does not stem from overfitting early data, which would result in correct value function on seen data, but rather from improperly fitting Q-values. 
% This can be traced to bootstrapping of poorly specified Q-function networks on unseen data. 
We demonstrate that this divergence is most likely caused by prediction of out-of-distribution (OOD) actions that trigger large gradient updates, compounded by the momentum terms in the Adam optimizer~\parencite{kingma2015adam}.
% While there are standard approaches that have been proposed to solve the issue of overestimation, our experiments highlight that these come with unwanted side effects in the high update-to-data ratio setting with limited data. 

The identified behavior, although triggered by OOD action prediction, seems to be more than the well-known overestimation due to statistical bias \parencite{thrun1993issues}. 
Instead, we hypothesize that the problem is an optimization failure and focus on limiting the exploding gradients from the optimizer via architectural changes.
The main evidence for this hypothesis is that standard RL approaches to mitigating bias, such as minimization over two independent critic estimates~\parencite{fujimoto2018addressing}, are insufficient. In addition, using pessimistic updates~\parencite{fujimoto2019bcq, fujimoto2021td3bc} or regularization~\parencite{krogh1991simple, srivastava14dropout} to treat the value divergence can potentially lead to suboptimal learning behavior, which is why architectural improvements are preferable in many cases.

% Combating overestimation is a problem with a long history in both value-based and actor-critic approaches, but our findings suggest that the most common approach (minimization over two independent critic estimates)  is insufficient, especially in the high UTD regime.

We use a simple feature normalization method \parencite{zhang2019root, wang2020striving, bjorck2022is} that projects features onto the unit sphere.
This decouples learning the scale of the values from the first layers of the network and moves it to the last linear layer.
Empirically, this approach fully mitigates diverging Q-values in the priming experiment. Even after a large amount of priming steps, the agent immediately starts to learn. %, rendering resetting unnecessary. 
In a set of experiments on the \textsf{dm\_control} MuJoCo benchmarks~\parencite{tunyasuvunakool2020dmcontrol}, we show that accounting for value divergence can achieve significant across-task performance improvements when using high update ratios. 
Moreover, we achieve non-trivial performance on the challenging dog tasks that are often only tackled using model-based approaches. We demonstrate comparable performance with the recently developed TD-MPC2~\parencite{hansen2024tdmpc}, without using models or advanced policy search methods.
Lastly, we isolate more independent failure modes, giving pointers towards their origins. In Appendix~\ref{app:open} we list open problems whose solutions might illuminate other RL optimization issues.


\section{Preliminaries} \label{sec:preliminaries}

\section{Investigating the effects of large update-to-data ratios during priming} \label{sec:investigating}

\begin{figure}[t!]
% \captionsetup[subfigure]{font=footnotesize, aboveskip=2pt}
\centering
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[height=0.4cm]{figures/dissecting/priming/priming_base_return_legend.pdf}
    \end{subfigure}\\%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=3.7cm, trim=1cm 1cm 1cm 1cm ,clip]{figures/dissecting/priming/priming_base_return.pdf}
        \label{subfig:priming_base_ret}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
    \centering
        \includegraphics[width=3.7cm, trim=1cm 1cm 1cm 1cm ,clip]{figures/dissecting/priming/priming_base_Q.pdf}
        \label{subfig:priming_base_Q}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=3.7cm, trim=1cm 1cm 1cm 1cm ,clip]{figures/dissecting/priming/priming_base_exp_avg.pdf}
        \label{subfig:priming_base_mom}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=3.7cm, trim=1cm 1cm 1cm 1cm ,clip]{figures/dissecting/priming/priming_base_exp_avg_sq.pdf}
        \label{subfig:priming_base_mom}
    \end{subfigure}%
    \caption{Return, in-distribution Q-values and Adam optimizer moments during priming for different lengths. Dotted lines correspond to end of priming. More priming leads to lower return and larger Q-value and optimizer divergence.}
    % \caption{Mean return, Q-values and Adam moments for priming of different lengths. Dotted lines indicate end of priming. More priming leads to lower return and Q-value and optimizer divergence.}
    \label{fig:priming_base}
\end{figure}

As mentioned, the definition of the primacy bias is broad.
To obtain a more nuanced understanding, we set out to re-investigate the early stages of high-UTD training. To do so, we repeat the priming experiment conducted by~\textcite{nikishin2022primacy}.\footnote{For consistency with later sections, we use the ReLU activation here which can lead to unstable learning of other components. We repeat all the experiments with ELUs in Appendix~\ref{app:priming} to provide even stronger support of our findings.}
We first collect a small amount of random samples. Then, using the SAC algorithm, we perform a priming step, training the agent for a large number of updates without additional data. After priming, training continues as usual. Prior results reported by~\textcite{nikishin2022primacy} suggest that once the priming step has happened, agents lose their ability to learn completely. We use the simple Finger-spin task \parencite{tunyasuvunakool2020dmcontrol} to study the root causes for this systematic failure and to examine if there are ways to recover without resets. In this section, we report means over five random seeds with standard error in shaded regions. Hyperparameters are kept consistent with previous work for ease of comparison.
% \mh{we should maybe add something about more stable results from elu in appendix}

\subsection{An old acquaintance: Q-value overestimation} \label{sec:overestimation}
We first ask whether there is a barrier as to how many steps an agent can be primed for before it becomes unable to learn. We test this by collecting 1,000 samples and varying the number of updates during priming from 25,000 to 50,000 and 100,000. The results are presented in Figure~\ref{fig:priming_base}. 

We make two key observations. First, lower amounts of priming are correlated with higher early performance. More precisely, it seems that many runs simply take longer before they start learning as the number of priming steps increases. Second, during priming the scale of the average Q-value estimates on observed state-action pairs increases drastically. We find that the Q-values start out at a reasonable level, but as priming goes on they eventually start to diverge drastically. Once the agent estimates very large Q-values, the final performance in terms of average returns deteriorates. We also observe that the second moment of the Adam optimizer~\parencite{kingma2015adam} is correlated with the divergence effect. Optimizer divergence has been observed before as a cause of plasticity loss under non-stationarity~\parencite{lyle2023understanding}, but in our experiments the data is stationary during priming. We conjecture that the momentum terms lead to much quicker propagation of poor Q-values and ultimately to prediction of incorrect Q-values, even on in-distribution data.

% However, without regularization it seems that once the effective dimension falls below a certain threshold, agents take substantially longer to recover and learning becomes essentially impossible (see Appendix~\ref{todo}). 

After priming, there exist two cases: 1) either the Q-values need to be unlearned before the agent can make progress or 2) there is a large drop from very high to very low Q-values that is strongly correlated with loss in effective dimension of the embedding, as defined by~\cite{yang2020harnessing} (see Appendix~\ref{app:priming_dim}). In the second case, rank can sometimes be recovered upon seeing new, counter-factual data and the network continues to learn. Yet, sometimes the agent gets stuck at low effective dimension; a possible explanation for the failure to learn observed in the priming experiments of~\textcite{nikishin2022primacy}. This is orthogonal to a previously studied phenomenon where target network-based updates lead to perpetually reduced effective rank~\parencite{kumar2021implicit}.

%However, the priming experiment alone does not allow us to conclude whether overestimation is a symptom or a cause of the primacy effect.
% To differentiate, we need a way to either prevent capacity loss in the network, or to regularize the Q values as to control for the impact of one or the other. 

% When looking at the resetting results in~\textcite{nikishin2022primacy, doro2023barrier}, several interesting observations can be made. First, it seems that in many cases a large part of the performance gains that are obtained from resetting seem to happen after the first reset. Additionally, after resetting for the first time, the variance across random seeds ostensibly reduces significantly in many of the experiments. \mh{Lastly, it seems unclear when exactly resetting which network should happen.} We set out to explore the cause for these events by considering a similar priming experiment as suggested in~\textcite{nikishin2022primacy}. We collect an initial sample of $2560$ observations from the environment, train for a large number of updates $U$ and this initial set and then return to standard SAC training. In doing so, we vary the number $U$ to artificially create the effects of increased UTD-ratios. 

% \subsection{Q-value divergence as a root cause}

% The results in~\ref{fig:priming} indicate that a key property of the training runs is that with increased $U$, 

% So as long as priming is not too strong, we can live with it. However, we must "unlearn" bad Q-values before we can start learning to solve the task. We hypothesize that resetting helps both to speed-up unlearning as well as getting back from the point of no return. We hypothesize that, if divergence never happens, we never have to do either of these and should simply be able to learn removing the need for expensive retraining after resets.
% Our core observation is that the priming procedure produces strong Q value - not only overestimation but - divergence which up to a certain extent can be combated with more training. 


\subsection{On the potential causes of divergence} \label{sec:causes}

\begin{figure}[t]
\begin{minipage}[b]{.35\textwidth}
% \captionsetup[subfigure]{font=footnotesize, aboveskip=2pt}
\centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[height=0.8cm]{figures/dissecting/priming/priming_causes_return_legend.pdf}
    \end{subfigure}\\%
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=4.8cm, trim=1cm 1cm 1cm 1cm ,clip]{figures/dissecting/priming/priming_causes_Q.pdf}
        \label{subfig:priming_causes_Q}
    \end{subfigure}%
    \caption{Priming with SAC and action regularization during priming. The latter lowers divergence. }
    \label{fig:priming_causes}
\end{minipage}
\hfill
\begin{minipage}[b]{.62\textwidth}
% \captionsetup[subfigure]{font=footnotesize, aboveskip=2pt}
    \centering
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[height=0.8cm]{figures/dissecting/priming/priming_ablations_return_legend.pdf}
    \end{subfigure}\\%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=4.8cm, trim=1cm 1cm 1cm 1cm ,clip]{figures/dissecting/priming/priming_ablations_return.pdf}
        \label{subfig:priming_abl_ret}
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
    \centering
        \includegraphics[width=4.8cm, trim=1cm 1cm 1cm 1cm ,clip]{figures/dissecting/priming/priming_ablations_Q.pdf}
        \label{subfig:priming_abl_Q}
    \end{subfigure}%
    \caption{Return and
    Q-values of priming runs with weight decay and dropout. Results indicate that both regularizations mitigate priming to some extent but not sufficiently. }
    \label{fig:priming_abl}
\end{minipage}
\end{figure}

% A well-known phenomenon that was already identified in the early stages of reinforcement learning research is that trying to find a value function via function approximation can quickly lead to overestimates of said values~\textcite{thrun1993issues}. This overestimation of the value function remains a challenge even for modern algorithms like SAC, due to the hurdles that emerge with what is often referred to as the \emph{deadly triad}: function approximation, reliance on bootstrap updates and off-policy data in the form of replay buffers. While this would naively suggest that these algorithms cannot work, empirical evidence strongly suggest that these algorithms are not impacted by the deadly triad to such an extent as to prevent good performance. 

We conjecture that Q-value divergence starts with overestimated values of OOD actions. This overestimation could cause the optimizer to continually increase Q-values via its momentum leading to divergence. 
%To test our hypothesis, we repeat the priming with 100,000 steps but perform an intervention regularizing the actions that the actor ouputs during priming (see Figure~\ref{fig:priming_causes}). 
% To dig deeper, we conjecture twofold: 1) Q-value divergence starts with overestimated values of OOD actions. 2) This overestimation triggers the optimizer to continually increase Q-values via the second-order optimizer momentum leading to divergence. To test our hypotheses, we repeat the priming with 100,000 steps but perform two interventions on the priming updates (see Figure~\ref{fig:priming_causes}). 
% \comE{It isn't 100\% clear how Fig 2 matches up with these interventions. The wording/names should match between Fig 2 and its caption and the interventions.}
To test this hypothesis, we add a conservative behavioral cloning~\parencite{pomerleau1988alvinn, atkeson1997robot} loss term to our actor that forces the policy to be close to replay buffer actions. Prior work employed this technique in offline RL to mitigate value overestimation~\parencite{fujimoto2021td3bc}. More formally, our actor update is extended by the loss $
        \mathcal{L}_{c, \psi} = \min_{\psi} \mathbb{E}_{a \sim \mathcal{D}, \hat{a} \sim \pi_{\psi}(s)}[||a - \hat{a}||_2]$. 
The results in Figure~\ref{fig:priming_causes} indicate that the basis of the conjecture is corroborated as divergence is much smaller---but not mitigated completely---when values are learned on actions similar to seen ones. However, in practice we do not know when divergence sets in, which limits the applicability of this technique in realistic scenarios. Using it throughout all of training, rather than just during priming, impairs the learner's ability to explore. We investigate the effects of the optimizer in more detail and provide preliminary evidence that the second-order term may be at fault in Appendix~\ref{app:priming_opt}. 
%For now, we focus on common techniques to mitigate concerns like weight or gradient divergence.

% {\bf Optimizer momentum}~~ We use standard stochastic gradient descent (SGD)~\parencite{robbins1951stochastic} with first-order momentum~\parencite{rumelhart1986learning, sutskever2013on} during priming to isolate the effect of the second-order momentum term. After priming, training continues with standard Adam optimization~\parencite{kingma2015adam}. As Figure~\ref{fig:priming_causes} shows, without  second-order momentum, divergence does not occur. The learner can solve the task without delay.


% \comE{ambiguous as to whether this is a goal of the paper/work, or whether this is future work}

\subsection{Applying common regularization techniques} \label{sec:regularization}
% \comE{It isn't clear how this section follows from what is before}

Regularization is a common way to mitigate gradient explosion and is  often used to address overestimation~\parencite{farebrother2018generalization, chen2021randomized, liu2021regularization, hiraoka2022dropout, li2023efficient}. We investigate the priming experiments under techniques such as using L$^2$ weight decay~\parencite{krogh1991simple} or adding dropout~\parencite{srivastava14dropout} to our networks in Figure~\ref{fig:priming_abl}.

Both L$^2$ weight decay as well as dropout can somewhat reduce the divergence during priming, however not to a sufficient degree.
While L$^2$ regularization fails to attain very high performance, dropout is able to recover a good amount of final return.
% L$^2$ regularization is hard to tune and can sometimes lead to severe underestimation.
However, both methods require tuning of a hyperparameter that trades off the regularization term with the main loss.
This hyperparameter is environment-dependent and tuning it becomes infeasible for large UTD-ratios due to computational resource limitations. 
% \comE{Rework last three sentences. Awkward writing, somewhat unclear connections between tuning L2 reg and the hyperparameter you allude to. Can you be more specific about the hyperparameter?} 
Still, the results imply that it is in fact possible to overcome the divergence in priming and continue to learn good policies.

% However, we find that using L$^2$ regularization, the Q-values estimates are significantly lower than they ought to be. The regularization reduces weights across all layers, reflecting a zero-centered prior for prediction. We anticipate this posing a challenge in harder tasks that require more exploration. 

% \paragraph{Entropy} The entropy regularizer in SAC is an additional factor that leads to less Q-value divergence. It is directly correlated to the Q-value magnitude and decreases the divergence effect. This goes in hand with decreasing the rank reduction effect which implies that divergence comes before rank reduction. \mh{only partially correct, needs updating}

% \paragraph{Target networks} \mh{probably remove this} A recent study has found that rank reduction can be produced by target network updates. We find that this is in fact a contributed factor to the large Q-value phenomenon and show that the exponential moving average update in Q-values simply leads to slower rank reduction during initial phases of training. However, we would like to update our target network quickly. Larger UTDs should help fit these networks correctly and them updating them quickly should lead to good learning. Large UTDs and moving average updates are effectively equivalent to lower UTD with large tau if we can fit the network properly.

% \paragraph{Big networks}. If our hypothesis are correct, then as long as we properly regularize and account for lost rank, we should be able to remove target networks. We demonstrate this by using larger Q-networks during high-UTD training and show that we can outperform standard resetting. Note that there is no difference between ema and tau=1 at this point due to the large amount of updates. We show competitive performance on various dm\_control tasks even on dog.

\subsection{Divergence in practice} 
%\subsection{Practical divergence} 
\label{sec:real}



One question that remains is whether we can find these divergence effects outside of the priming setup. We find that, while priming is an artificially constructed worst case, similar phenomena happen in regular training on standard benchmarks when increasing update ratios (see Figure~\ref{fig:humanoid_failure}). Further, the divergence is not limited to early stages of training as it happens at arbitrary points in time.
We therefore conjecture that divergence is not a function of 
\begin{wrapfigure}{r}{0.54\textwidth}
  \begin{minipage}{\linewidth}
    \centering\captionsetup[subfigure]{justification=centering}
    \includegraphics[width=4cm, trim=1cm 1cm 1cm 1cm ,clip]{figures/dissecting/priming/humanoid_Q.pdf}\vspace{-8pt}
    \includegraphics[width=4cm, trim=1cm 1cm 1cm 1cm ,clip]{figures/dissecting/priming/humanoid_loss.pdf}
    \caption{In-distribution Q-values and critic loss of five SAC seeds on the humanoid-run task using $\mathrm{UTD}=32$. Values diverge at arbitrary time-points, not only during the beginning. Loss mirrors Q-value divergence.}
    \label{fig:humanoid_failure}
  \end{minipage}
\end{wrapfigure}
amount of experience but rather one of state-action space coverage. Note that the reported Q-values have been measured on the observed training data, not on any out-of-distribution state-action pairs. 
The respective critic losses become very large. All this points toward a failure to fit Q-values. This behavior does not align with our common understanding of overfitting~\parencite{bishop2006pattern},  challenging the hypothesis that high-UTD learning fails merely due to large validation error~\parencite{li2023efficient}.

\section{Towards high-UTD optimization without resetting} \label{sec:method}

Regularization techniques such as those in Section~\ref{sec:regularization} can fail to alleviate divergence as they tend to operate across the whole network and lower the weights everywhere even if higher values are actually indicated by the data. They also require costly hyperparameter tuning. Thus, we turn towards network architecture changes to the commonly used MLPs that have proven useful in overcoming issues such as exploding gradients in other domains~\parencite{ba2016layer, xu2019understanding}. 

\subsection{Limiting gradient explosion via unit ball normalization} \label{sec:unitball}

\begin{figure}[t]
\begin{minipage}[t]{.60\textwidth}
% \captionsetup[subfigure]{font=footnotesize, aboveskip=2pt}
\centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[height=0.4cm]{figures/dissecting/priming/priming_norm_return_legend.pdf}
    \end{subfigure}\\%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        %\setlength{\fboxsep}{0pt}\fbox{
        \includegraphics[width=4.8cm]{figures/dissecting/priming/priming_norm_return.pdf}
        \label{subfig:priming_norm_ret}
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=4.8cm]{figures/dissecting/priming/priming_norm_Q.pdf}
        \label{subfig:priming_norm_Q}
    \end{subfigure}%
    \caption{(Left) Return and (Right) Q-values comparing SGD result and OFN when priming for 100K steps. OFN obtains returns close to that of the well-trained SGD agent and learns an appropriate Q-value scale correctly.}
    \label{fig:priming_norm}
\end{minipage}
\hfill
\begin{minipage}[t]{.36\textwidth}
% \captionsetup[subfigure]{font=footnotesize, aboveskip=2pt}
\centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[height=0.8cm]{figures/dissecting/priming/weight_magnitudes_legend.pdf}
    \end{subfigure}
    \\%
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=4.6cm]{figures/dissecting/priming/weight_magnitudes.pdf}
    \end{subfigure}%
    \caption{$L_2$ norm of network weights per layer after priming for default and OFN  architectures. OFN leads to smaller weights and significant mass in the last layer.}
    \label{fig:weight_norm}
\end{minipage}
\end{figure}

As discussed previously, the prediction of an unknown action might trigger the propagation of a large, harmful gradient. Further, the Q-values of our network ought to grow over time as they more closely approximate those of a good policy. If we predict incorrectly on one of these Q-values, a potentially very large loss is propagated. Gradients are magnified by multiplicative backpropagation via ReLU activations~\parencite{glorot2011deep} as well as momentum from Adam~\parencite{kingma2015adam}. Note that all resulting issues arise in the early network layers. 
We hypothesize that we can address many of these problems by separating the scaling of the Q-values to the appropriate size from the earlier non-linear layers of the network and moving the Q-value scaling to the final linear layer.
%should decouple learning to differentiate Q-values of different actions from scaling the Q-values to the correct magnitude. 
%The former, we want to do in earlier non-linear stages of the network while the latter should solely happen via the last linear transformation.

One contender to achieve the value decoupling described in the previous paragraph is layer normalization~\parencite{ba2016layer}, but one would have to disable scaling factors used in common implementations. Still, standard layer normalization would not guarantee small features everywhere. Instead, we use a stronger constraint and project the output features of the critic encoder onto the unit ball using the function
$f(\mathbf{x}) = \frac{\mathbf{x} }{\|\mathbf{x}\|_2}$~\parencite{zhang2019root}, 
where $\|\cdot\|_2$ denotes the L$^2$ vector norm and $\mathbf{x}$ is the output of our encoder $\phi(s, a)$. This ensures that all values are strictly between $0$ and $1$ and the gradients will be tangent to the unit sphere. Note that this function's gradient is not necessarily bounded to ensure low gradient propagation (see Appendix~\ref{app:unitnorm}), but we argue that if large values are never created in the early layers, gradient explosion will not occur. The unit ball has previously been used to mitigate large action prediction in the actor~\parencite{wang2020striving} or to stabilize RL training in general~\parencite{bjorck2022is}. 
For brevity, we will refer to this method as {\em output feature normalization} (OFN). We solely apply OFN to the critic, unlike~\textcite{wang2020striving}, since our goal is to mitigate value divergence. OFN is very simple and requires only a one-line change in implementation.

% We note the simplicity of the approach as it essentially requires a one-line change in implementation.

% A common technique for normalization of intermediate embeddings is Layer normzliation....
% However, here we make a crucial design choice. Common implementations of layer normalization are implemented via the following rule
% \begin{equation*}
%     l
% \end{equation*}
% where... . We need to ensure that scaling cannot be propagated into our earlier layers and need to disable the learnable parameters of this normalizer. We ablate this choice in Appendix~\ref{todo}


\subsection{Evaluating feature output normalization during priming} \label{sec:evalmethod}

To test the efficacy of the OFN-based approach, we repeat the priming experiment in Figure~\ref{fig:priming_norm}.
We find that OFN achieves high reward and most distinctly, Q-value divergence during priming is fully mitigated. 
Note also that we are using a discount factor of $\gamma = 0.99$, returns are collected over 1,000 timesteps and rewards are in $[0, 1]$. 
We therefore expect the average Q-values to be roughly at $10\%$ of the undiscounted return which seems correct for the OFN network. 
However, more importantly, as shown in Figure~\ref{fig:weight_norm}, most of the Q-value scaling happens in the last layer.

\section{Experimental evaluation}

We evaluate our findings on the commonly used \textsf{dm\_control} suite~\parencite{tunyasuvunakool2020dmcontrol}. All results are averaged over ten random seeds.\footnote{For comparison with TD-MPC2~\parencite{hansen2024tdmpc} we use data provided by their implementation, which only contains three seeds. As the goal is not to rank algorithmic performance but to give intuition about the relative strengths of adapting the network architecture, we believe that this is sufficient in this case.} We report evaluation returns similar to~\textcite{nikishin2022primacy}, which we record every 10,000 environment steps. We compare a standard two-layer MLP with ReLU ~\parencite{nair2010rectified} activations, both with and without resetting, to the same MLP with OFN. %to achieve value clipping. 
The architecture is standard in many reference implementations. Architecture and the resetting protocol are taken from \textcite{doro2023barrier} and hyperparameters are kept without new tuning to ensure comparability of the results. More details can be found in Appendix~\ref{app:impl}.

To understand the efficacy of output normalization on real environments under high UTD ratios, we set out to answer multiple questions that will illuminate RL optimization failures:\\
{\bf Q1:}~Can we maintain learning without resetting neural networks?\\
{\bf Q2:}~Are there other failure modes beside Q-value divergence under high UTD ratios?\\
{\bf Q3:}~When resets alone fall short, can architectural changes enable better high-UTD training?

\subsection{Feature normalization stabilizes high-UTD training} 

\begin{figure}[t]
\begin{minipage}[b]{0.67\textwidth}
% \captionsetup[subfigure]{font=footnotesize, aboveskip=2pt}
\centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        %\setlength{\fboxsep}{0pt}\fbox{
        \includegraphics[width=10.2cm]{figures/dissecting/main_exp/all_aggregate_scores.pdf}
        %}
    \end{subfigure}%
    %\vspace{-.8em}
    \caption{Mean, interquartile mean (IQM), and median with $95\%$ bootstrapped confidence intervals of standard SAC and OFN on the DMC15-500k Suite. OFN can maintain high performance even under large UTD. OFN with $\mathrm{UTD} = 8$ achieves comparable performance to standard resetting with $\mathrm{UTD} = 32$ across metrics.}
    \label{fig:aggregate}
\end{minipage}
\hfill
\begin{minipage}[b]{.3\textwidth}
% \captionsetup[subfigure]{font=footnotesize, aboveskip=2pt}
\centering
    % %\vspace{-117pt} 
    % \begin{subfigure}[b]{\textwidth}
    %     %\hspace{15pt}
    %     \centering
    %     %\setlength{\fboxsep}{0pt}\fbox{
    %     \includegraphics[height=0.8cm]{figures/dissecting/main_exp/hopper_hop_legend.pdf}
    %     %}
    % \end{subfigure}
    % \\%
    \begin{subfigure}[b]{\textwidth}
        \centering
        %\setlength{\fboxsep}{0pt}\fbox{
        \includegraphics[width=4.25cm]{figures/dissecting/main_exp/hopper_hop_wl.pdf}
         %}
        %\vspace{10pt}
        %\vspace{-1em}
    \end{subfigure}%
    %\vspace{-.8em}
    \caption{Mean return of single-critic OFN, standard OFN and resetting; $\mathrm{UTD}=32$ on hopper-hop. Shaded regions are standard error.}
    \label{fig:hopper_hop}
\end{minipage}
\end{figure}

% \begin{figure}[t!]
% \begin{minipage}[b]{.52\textwidth}
% % \captionsetup[subfigure]{font=footnotesize, aboveskip=2pt}
% \centering
%     \begin{subfigure}[b]{\textwidth}
%         \centering
%         %\setlength{\fboxsep}{0pt}\fbox{
%         \includegraphics[width=7.3cm,clip,trim=0.3cm 0 0 1cm]{figures/dissecting/main_exp/aggregate_scores.pdf}
%         %}
%     \end{subfigure}%
%     \vspace{-8pt}
%     \caption{Interquartile mean and $95\%$ bootstrapped confidence intervals of high-UTD standard SAC and OFN on the DMC15-500k Suite. OFN can maintain high performance even under large update ratios.}
%     \label{fig:aggregate}
% \end{minipage}
% \hfill
% \begin{minipage}[b]{.45\textwidth}
% % \captionsetup[subfigure]{font=footnotesize, aboveskip=2pt}
% \centering
%     \begin{subfigure}[b]{\textwidth}
%         \centering
%         %\setlength{\fboxsep}{0pt}\fbox{
%         \includegraphics[width=4.6cm,clip,trim=0 0 0 0]{figures/dissecting/main_exp/hopper_hop.pdf}
%         \includegraphics[height=0.6cm,clip,trim=0cm 0cm 15cm 0cm]{figures/dissecting/main_exp/hopper_hop_legend.pdf}
%         %}
%         \vspace{-8pt}
%     \end{subfigure}%
%     \caption{Single-critic OFN versus standard OFN and standard resetting with $\mathrm{UTD}=32$ on the hopper-hop task. With only a single critic, OFN strongly outperforms resetting.}
%     \label{fig:hopper_hop}
% \end{minipage}
% \vspace{-5pt}
% \end{figure}

% \begin{figure}{r}{0.5\textwidth}
%     \vspace{-12pt}
%     \centering
%     \includegraphics[width=7.3cm, trim={0.3cm 0.3cm 0.3cm 0}, clip]{figures/dissecting/main_exp/aggregate_scores.pdf} 
%     \caption{IQM}
%     \label{fig:aggregate}
%     \vspace{-5pt}
% \end{figure}

To answer \textbf{Q1}, we compare OFN and SAC with resets on the DMC15-500k benchmark with large update ratios of $8$ and $32$ as proposed by \textcite{nikishin2022primacy} and  \textcite{schwarzer2023bigger}.  We report mean, interquartile mean (IQM) and median as well as $95\%$ bootstrapped confidence intervals aggregated over seeds and tasks,  following~\textcite{agarwal2021deep}. The results are shown in Figure~\ref{fig:aggregate}.

First, we observe that in both cases, $\mathrm{UTD}=8$ and $\mathrm{UTD}=32$, OFN can significantly improve over the non-resetting MLP baseline across all metrics. The value estimates that diverge seem to have been handeled properly (see Appendix~\ref{app:exp_q}); learning is maintained. We note that our approach with $\mathrm{UTD}=8$ achieves mean and IQM performance comparable to that of standard resetting with $\mathrm{UTD}=32$. In median and quantile performance, all $\mathrm{UTD}=32$ overlap, highlighting that outliers contribute to the performance measurement. Note that the overall performance drops slightly for the OFN-based approach when going from $\mathrm{UTD}=8$ to $\mathrm{UTD}=32$. We conjecture that there are other learning problems such as exploration that have not been treated by alleviating value divergence. However, these do not lead to complete failure to learn but rather slightly slower convergence. % \comE{unclear last sentence}

\subsection{Other failure modes: Exploration limitations} \label{sec:otherfailure}

% \begin{wrapfigure}{r}{0.36\textwidth}
%     \vspace{-45pt}
%     \centering
%     \hspace{5pt}
%     \includegraphics[height=0.7cm]{figures/dissecting/main_exp/hopper_hop_legend.pdf} \\
%     \includegraphics[width=4.8cm, trim={0.3cm 0cm 0.3cm 0.2cm}, clip]{figures/dissecting/main_exp/hopper_hop.pdf} 
%     \vspace{-5pt}
%     \caption{Comparison of single-critic OFN approach to standard OFN and standard resetting with $\mathrm{UTD}=32$ on the hopper-hop task. With only a single critic, OFN strongly outperforms resetting.}
%     \label{fig:hopper}
%     \vspace{-20pt}
% \end{wrapfigure}

To validate the hypothesis of other failures and answer \textbf{Q2}, we run two additional experiments. First, our current focus is on failures of the critic; our proposed mitigation does not address any further failures that might stem from the actor. We defer a more detailed analysis of actor failure cases to future work. Instead, we test the OFN-based architecture again and, for now, simply reset the actor to shed light on the existence of potential additional challenges.
%\comE{Need to better sell that this is ok, and it doesn't violate the premise of the paper of avoiding resetting. Hold the reader's hand}
For comparison, we also run a second experiment in which we reset all learned parameters, including the critic.
% The results are included. \comE{Last sentence is unclear. Included where? Cut, since Figure is mentioned in next sentence?}

The results in Figure~\ref{fig:aggregate} indicate that actor resetting can account for a meaningful portion of OFN's performance decline when going from $\mathrm{UTD}=8$ to $\mathrm{UTD}=32$. The actor-reset results are within variance of the full-resetting standard MLP baseline. Further, we observe that there is still some additional benefit to resetting the critic as well. 
%\comE{Again, need to sell that this is ok.} 
This does not invalidate the premise of our hypothesis, value divergence might not be the \emph{only} cause of problems in the high UTD case. We have provided significant evidence that it is a \emph{major} contributor.
Resetting both networks of OFN with $\mathrm{UTD}=32$  outperforms all other baselines on mean and IQM comparisons. %We set out to study where the improvements induced by critic resets come from.

% \begin{wrapfigure}{r}{0.35\textwidth}
% % \captionsetup[subfigure]{font=footnotesize, aboveskip=2pt}
%   \begin{minipage}{\linewidth}
%     \centering\captionsetup[subfigure]{justification=centering}
%     \includegraphics[width=5.4cm, trim=0.3cm 0cm 0cm 0cm ,clip]{figures/dissecting/main_exp/hopper_hop.pdf}
%     \caption{Single-critic OFN versus standard OFN and resetting, with $\mathrm{UTD}=32$ on the hopper-hop task. }
%     \label{fig:hopper_hop}
%   \end{minipage}
%   \vspace{-8pt}
% \end{wrapfigure}

% \begin{wrapfigure}{r}{0.35\textwidth}
%     \begin{minipage}
%     % \captionsetup[subfigure]{font=footnotesize, aboveskip=2pt}
%         % %\vspace{-117pt} 
%         % \begin{subfigure}[b]{\textwidth}
%         %     %\hspace{15pt}
%         %     \centering
%         %     %\setlength{\fboxsep}{0pt}\fbox{
%         %     \includegraphics[height=0.8cm]{figures/dissecting/main_exp/hopper_hop_legend.pdf}
%         %     %}
%         % \end{subfigure}
%         % \\%
%             \centering
%             \setlength{\fboxsep}{0pt}\fbox{
%             \includegraphics[width=4.25cm,clip,trim=0.3cm 0 0 0]{figures/dissecting/main_exp/hopper_hop_wl.pdf}
%              }
%         \caption{Single-critic OFN versus standard OFN and resetting, with $\mathrm{UTD}=32$ on the hopper-hop task. }
%         \label{fig:hopper_hop}
%     \end{minipage}
% \end{wrapfigure}

To explain the remaining efficacy of critic resets, we examine the hopper-hop environment where standard SAC with resets outperforms OFN. 
%We conjecture that there is an additional exploration benefit inherit to resetting. 
In RL with function approximation, one might not only encounter over-  but also under-estimation~\parencite{wu2020reducing, lan2020maxmin, saglam2021estimation}.
We believe that hopper  is sensitive to pessimism, and periodically resetting the networks might partially and temporarily counteract the inherent pessimism of the dual critic setup.
%Suppose the critic's  overestimation has been handled properly but due to, there are unseen regions of the state-action space whose values are being underestimated. When the network is reset, some regions might now temporarily become overestimated regions, even if we train on the exact same data, which restarts the exploration process.

To obtain evidence for this conjecture, we repeated some experiments with a single critic. As OFN handles divergence it might not require a minimization over two critics~\parencite{fujimoto2018addressing}. We compare OFN using a single critic and $32$ updates per environment step to standard SAC and OFN in Figure~\ref{fig:hopper_hop}. With a single critic, OFN does not get stuck in a local minimum and outperforms full resetting. Note that this is only true in few environments, leading us to believe that the effects of high-update training are MDP-dependent.
In some environments we observe unstable learning with a single critic, which highlights that the bias countered by the double critic optimization and the overestimation from optimization we study are likely orthogonal phenomena that both need to be addressed.
Most likely, there is a difficult trade-off between optimization stability and encouraging sufficient exploration, which is an exciting avenue for future research.

\subsection{Limit-testing feature normalization}

\begin{figure}[t]
% \captionsetup[subfigure]{font=footnotesize, aboveskip=2pt}
\centering
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[height=0.4cm]{figures/dissecting/dog_exp/dog_legend.pdf}
    \end{subfigure}\\%
    \begin{subfigure}[t]{0.25\textwidth}
        \centering
        % \includegraphics[width=4.cm, trim=0.4 0 0 0 ,clip]{figures/dissecting/dog_exp/dog-stand.pdf}
        \label{subfig:dog_stand}
        % \caption{Dog Stand}
    \end{subfigure}%
    \begin{subfigure}[t]{0.25\textwidth}
        \centering
        % \includegraphics[width=3.7cm, trim=1.2cm 0 0 0 ,clip]{figures/dissecting/dog_exp/dog-walk.pdf}
        \label{subfig:dog_walk}
        % \caption{Dog Walk}
    \end{subfigure}%
    \begin{subfigure}[t]{0.25\textwidth}
    \centering
        % \includegraphics[width=3.7cm, trim=1.2cm 0 0 0 ,clip]{figures/dissecting/dog_exp/dog-trot.pdf}
        \label{subfig:dog_trot}
        % \caption{Dog Trot}
    \end{subfigure}%
    \begin{subfigure}[t]{0.25\textwidth}
        \centering
        % \includegraphics[width=3.7cm, trim=1.2cm 0 0 0 ,clip]{figures/dissecting/dog_exp/dog-run.pdf}
        \label{subfig:dog_run}
        % \caption{RGRDog Run}
    \end{subfigure}%
    % \vspace{-5pt}
    \caption{Mean return on the dog DMC tasks, comparing OFN to SAC with resets and the model-based TD-MPC2. Shaded regions indicate standard error. OFN outperforms SAC with resets, which is unable to learn and OFN with $\mathrm{UTD}=8$ and resetting is competitive with TD-MPC2.}
    \label{fig:all_dog}
\end{figure}

To answer \textbf{Q3}, we move to a set of training environments that is considered exceptionally hard for model-free approaches, namely the dog tasks of the DMC suite. Standard SAC can generally not obtain any reasonable reward and, due to their complex dynamics, these tasks are often tackled using model-based approaches such as TD-MPC2~\parencite{hansen2024tdmpc} with complicated update procedures and carefully tuned network architectures. We evaluate SAC and OFN on the dog tasks and compare against TD-MPC2 in Figure~\ref{fig:all_dog}.

First, observe that resetting without OFN obtains no improvement over a random policy. However, OFN with $\mathrm{UTD}=1$ can already obtain very good performance across all tasks, indicating that a major problem for SAC in these high-dimensional tasks is value divergence. When increasing the update ratio to $8$ and adding resetting, we improve the performance of the OFN agent even further and can match the reported results of the strong model-based TD-MPC2 baseline. 

We have already seen that resetting can take care of multiple optimization failures. However, these experiments also indicate that resetting is not a panacea as it is only effective when the initially learned policy can obtain some reward before being overwritten. 
This seems intuitive since resetting to a policy that cannot gather any useful data should not help. 
These results highlight that the early training dynamics of RL are highly important when it comes to training on complex environments and fitting early data correctly and quickly is crucial for success. 

This also opens up the question why resetting in the humanoid environments in the previous sections can yield success even though very little reward is observed. 
Besides greater divergence due to larger observation spaces in the dog MDPs, we suspect that this might be related to the complexity of exploration.
The ability of a random policy to obtain non-trivial reward and information about the environment has been shown to be a crucial factor in explaining the success of DRL methods in discrete environments~\parencite{laidlaw2023bridging}, and similar phenomena might be in effect here. 

% \comE{\# seeds/trials?  Might be space to add in caption of Fig 9}

% \mh{tie this back to primacy bias, if resetting fixes the primacy bias, it should work here.} \mh{talk about environment specific study and mention bridging paper here} 


\section{Related work}

Our work closely examines previous work on the primacy bias and the related resetting technique \parencite{anderson1992qlearning,nikishin2022primacy,doro2023barrier,schwarzer2023bigger}.
Going beyond, overestimation and feature learning challenges are a widely studied phenomenon in the literature.

{\bf Combatting overestimation}~~
Overestimation in off-policy value function learning is a well-established problem in the RL literature that dates back far before the prime times of deep learning~\parencite{thrun1993issues, precup2001off}. The effects of function approximation error and the effect on variance and bias have been studied~\parencite{pendrith1997estimator, mannor2007bias} as well.
With the rise of deep learning, researchers have tried to address the overestimation bias via algorithmic interventions such as combining multiple Q-learning predictors to achieve underestimation~\parencite{hasselt2010double, hasselt2016deep, zongzhang2017weighted, lan2020maxmin}, using averages over previous Q-values for variance reduction~\parencite{anschel2017averaged}, or general error term correction~\parencite{donghun2013bias, fox2016taming}. In the context of actor-critic methods, the twinned critic minimization approach of \textcite{fujimoto2018addressing} has become a de-facto standard. Most of these approaches are not applicable or break down under very high update ratios.
To regulate the overestimation-pessimism balance more carefully, several authors have attempted to use larger ensembles of independent Q-value estimates \parencite{lee2021sunrise,peer2021ensemble,chen2021randomized,hiraoka2022dropout}. Ensembling ideas were also combined with ideas from distributional RL~\parencite{bellemare2017distributional} to combat overestimation~\parencite{kuznetsov2020controlling}.  Instead of addressing the statistical bias in deep RL, our study focuses on the problems inherent to neural networks and gradient based optimization for value function estimation. Work from offline-to-online RL has demonstrated that standard layer normalization can bound value estimates during offline training and mitigate extrapolation while still allowing for exploration afterwards~\parencite{ball2023efficient}. Layer normalization has subsequently been used to achieve generally strong results in offline RL~\parencite{tarasov2023rebrac}. Our work is also related to a recent contribution using batch-normalization for increased computational efficiency by~\textcite{bhatt2024crossq} who focus on decreasing update ratios rather than increasing them. A concurrent work by~\textcite{nauman2024overestimation} provides a large scale study on different regularization techniques to combat overestimation. This work also demonstrates the efficacy of SAC on the dog tasks when properly regularized but it does not highlight the effects of Q-value divergence from exploding gradients as a key challenge for this set of environments.

{\bf Combating plasticity loss}~~ Another aspect of the primacy bias is the tendency of neural networks to lose their capacity for learning over time~\parencite{igl2021transient}, sometimes termed \emph{plasticity loss}~\parencite{lyle2021understanding, abbas2023loss}. Recent work mitigates plasticity loss using feature rank maximization~\parencite{kumar2021implicit}, regularization~\parencite{lyle2023understanding}, or learning a second copied network~\parencite{nikishin2024deep}. Some of the loss stems from neurons falling dormant over time~\parencite{sokar2023dormant}. A concurrent, closely related work by~\textcite{lyle2024disentangling} disentangles the causes for plasticity loss further. They use layer normalization to prevent some of these causes, which is closely related to our unit ball normalization. Our work differs in that we focus on the setting of high update ratios and use stronger constraints to mitigate value divergence rather than plasticity loss. 

% \textcite{lyle2021understanding} present an alternative regularization to ours which aims to keep the parameters of the network close to initialization.
% Similar to $L_2$ regularization, this introduces a difficult to tune hyperparameter and can prevent learning in the worst case.
% \textcite{nikishin2024deep} show that by learning a second copied network instead of resetting the original network, capacity loss can be prevented.
% However, this method leads to more and more networks over time, as old ones are frozen and no ones initialized, which is inefficient.
% Closely related is the idea of the effective rank of the feature matrix, which captures how strongly correlated certain features are across different samples.
% \textcite{kumar2021implicit} show that this can be regularized explicitly by using an orthogonal loss on the intermediate layers of the network, as this loss requires large correlation matrices, it is much more computationally expensive than simple normalization of the features.

\section{Conclusion and future work}

By dissecting the effects underlying the primacy bias, we have identified a crucial challenge: \emph{value divergence}. 
While the main focus in studying increased Q-value has been on the statistical bias inherent in off-policy sampling, we show that Q-value divergence can arise due to problems inherent to neural network optimization.
This optimization-caused divergence can be mitigated using the unit-ball normalization approach, which shines on the \textsf{dm\_control} benchmark with its simplicity and efficacy. 
With this result, we challenge the assumption that failure to learn in high-UTD settings primarily stems from \emph{overfitting} early data by showing that combating value divergence is competitive with resetting networks. 
This offers a starting point towards explaining the challenges of high-UTD training in more detail and opens the path towards even more performant and sample efficient RL in the future.

However, as our other experiments show, mitigating value overestimation through optimization is not the only problem that plagues high-UTD learning. 
To clearly highlight these possible directions for future work, we provide an extensive discussion of open problems in Appendix~\ref{app:open}. 
Additional problems, such as \emph{exploration failures} or \emph{suboptimal feature learning}, can still exist and need to be resolved to unlock the full potential of high-UTD RL.
