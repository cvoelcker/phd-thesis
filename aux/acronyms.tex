% General ML terminology
\acrodef{ml}[ML]{Machine Learning}
\acrodef{sgd}[SGD]{Stochastic Gradient Descent}
\acrodef{gd}[GD]{Gradient Descent}
\acrodef{mle}[MLE]{Maximum Likelihood Estimation}
\acrodef{mse}[MSE]{Mean Squared Error}
\acrodef{kl}[KL]{Kullback-Leibler Divergence}
\acrodef{nn}[NN]{Neural Network}
\acrodef{erm}[ERM]{Empirical Risk Minimization}

% RL fundamentals
\acrodef{rl}[RL]{Reinforcement Learning}
\acrodef{mbrl}[MBRL]{Model Based Reinforcement Learning}
\acrodef{mdp}[MDP]{Markov Decision Process}
\acrodef{pomdp}[POMDP]{Partially Observable Markov Decision Process}
\acrodef{vi}[VI]{Value Iteration}
\acrodef{pi}[PI]{Policy Iteration}
\acrodef{drl}[DRL]{Deep Reinforcement Learning}
\acrodef{dpg}[DPG]{Deterministic Policy Gradient}

% Algos
\acrodef{sac}[SAC]{Soft Actor Critic}
\acrodef{ppo}[PPO]{Proximal Policy Optimization}
\acrodef{dqn}[DQN]{Deep Q-Network}
\acrodef{td3}[TD3]{Twin Delayed Deep Deterministic Policy Gradient}
\acrodef{vaml}[VAML]{Value Aware Model Learning}
\acrodef{itervaml}[IterVAML]{Iterative Value Aware Model Learning}
\acrodef{vagram}[VaGrAM]{Value Gradient Aware Model Learning}
\acrodef{madtd}[MAD-TD]{Model Augemented Data for TD Learning}
\acrodef{cvaml}[C-VAML]{Corrected IterVAML}
\acrodef{mve}[MVE]{Model Value Expansion}
